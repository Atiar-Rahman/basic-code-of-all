{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install deep_sort_realtime\n",
        "!pip install tensorflow\n",
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-p_YFMzUSER",
        "outputId": "d5f68c40-5885-45b5-aec9-a65070c1da5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.202-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.202-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.202 ultralytics-thop-2.0.17\n",
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (1.16.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from deep_sort_realtime) (4.12.0.88)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_sort_realtime\n",
            "Successfully installed deep_sort_realtime-1.3.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "tMFddZ7ST0Lz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_frames(video_path, num_frames=16, target_size=(64,64)):\n",
        "    \"\"\"\n",
        "    Load `num_frames` evenly spaced frames from a video, resize, and normalize.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip = max(total_frames // num_frames, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i*skip)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, target_size)\n",
        "        frame = frame / 255.0          # normalize 0-1\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # If frames are less than num_frames, pad with last frame\n",
        "    while len(frames) < num_frames:\n",
        "        frames.append(frames[-1])\n",
        "\n",
        "    return np.array(frames)  # shape: (num_frames, H, W, C)\n"
      ],
      "metadata": {
        "id": "NZkdwYIiU0kJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/data1\"\n",
        "num_frames = 16\n",
        "target_size = (64,64)\n",
        "\n",
        "X, y = [], []\n",
        "class_names = sorted(os.listdir(dataset_dir))\n",
        "\n",
        "for label, class_name in enumerate(class_names):\n",
        "    class_folder = os.path.join(dataset_dir, class_name)\n",
        "    for video_file in os.listdir(class_folder):\n",
        "        video_path = os.path.join(class_folder, video_file)\n",
        "        frames = load_video_frames(video_path, num_frames=num_frames, target_size=target_size)\n",
        "        X.append(frames)\n",
        "        y.append(label)\n",
        "\n",
        "X = np.array(X)  # shape: (num_videos, num_frames, H, W, C)\n",
        "y = np.array(y)\n",
        "print(\"Dataset shape:\", X.shape, y.shape)\n",
        "print(\"Classes:\", class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnqGWDuUU4w7",
        "outputId": "5985c668-3456-4c03-e479-60abc90a5afa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (163, 16, 64, 64, 3) (163,)\n",
            "Classes: ['a', 'f', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "cNtSvXzNU9BV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_sFwEuuBTXr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57e3382-77b0-4f77-b1be-40e1330e9c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 64.0MB/s 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# -----------------------\n",
        "# 1. Setup models\n",
        "# -----------------------\n",
        "yolo_model = YOLO(\"yolov8n.pt\")   # detects humans\n",
        "tracker = DeepSort(max_age=30)    # tracks IDs\n",
        "\n",
        "# -----------------------\n",
        "# 2. 3D CNN definition\n",
        "# -----------------------\n",
        "num_frames = 16\n",
        "img_size = (64,64)\n",
        "num_classes = 5   # adjust to your dataset\n",
        "\n",
        "input_shape = (num_frames, img_size[0], img_size[1], 3)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv3D(32, (3,3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling3D((1,2,2)),\n",
        "    layers.Conv3D(64, (3,3,3), activation='relu'),\n",
        "    layers.MaxPooling3D((2,2,2)),\n",
        "    layers.Conv3D(128, (3,3,3), activation='relu'),\n",
        "    layers.MaxPooling3D((2,2,2)),\n",
        "    layers.Conv3D(256, (1,2,2), activation='relu'),  # Changed kernel size again\n",
        "    layers.MaxPooling3D((2,2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # -----------------------\n",
        "# # 3. Video Processing\n",
        "# # -----------------------\n",
        "# def process_video(video_path):\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     tracks_dict = {}   # store frame sequences per person ID\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         # Detect people\n",
        "#         results = yolo_model.predict(frame, classes=[0]) # class 0 = person\n",
        "#         detections = []\n",
        "#         for r in results:\n",
        "#             for box in r.boxes.xyxy.cpu().numpy():\n",
        "#                 x1,y1,x2,y2 = box[:4]\n",
        "#                 detections.append(([x1,y1,x2-x1,y2-y1], 1.0, 'person'))\n",
        "\n",
        "#         # Track people\n",
        "#         tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "#         for track in tracks:\n",
        "#             if not track.is_confirmed():\n",
        "#                 continue\n",
        "#             track_id = track.track_id\n",
        "#             l,t,r,b = map(int, track.to_ltrb())\n",
        "#             crop = frame[t:b, l:r]\n",
        "#             if crop.size == 0:\n",
        "#                 continue\n",
        "#             crop = cv2.resize(crop, img_size)\n",
        "#             crop = crop/255.0\n",
        "\n",
        "#             if track_id not in tracks_dict:\n",
        "#                 tracks_dict[track_id] = []\n",
        "#             tracks_dict[track_id].append(crop)\n",
        "\n",
        "#             # If enough frames collected, classify\n",
        "#             if len(tracks_dict[track_id]) >= num_frames:\n",
        "#                 clip = np.array(tracks_dict[track_id][:num_frames])\n",
        "#                 clip = np.expand_dims(clip, axis=0)  # batch dimension\n",
        "#                 pred = model.predict(clip)\n",
        "#                 print(f\"Person {track_id} -> Predicted class: {np.argmax(pred)}\")\n",
        "\n",
        "#                 # Optionally keep overlap frames\n",
        "#                 tracks_dict[track_id] = tracks_dict[track_id][8:]\n",
        "\n",
        "#     cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "fWHBpCfcTytJ",
        "outputId": "d7cd34c6-e8e4-4ea6-ed87-e27b0cc0a980"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m) │         \u001b[38;5;34m2,624\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m55,360\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m) │       \u001b[38;5;34m221,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_2 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_3 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m1,285\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m674,309\u001b[0m (2.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">674,309</span> (2.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m674,309\u001b[0m (2.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">674,309</span> (2.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFRghWOwkMKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFLy-I1STkzb",
        "outputId": "3636f1a4-e49f-42a5-af2c-8b7791923aa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - accuracy: 0.3374 - loss: 1.3522 - val_accuracy: 0.3939 - val_loss: 1.0974\n",
            "Epoch 2/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.2943 - loss: 1.1953 - val_accuracy: 0.3030 - val_loss: 1.1748\n",
            "Epoch 3/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.3063 - loss: 1.1519 - val_accuracy: 0.4545 - val_loss: 1.1300\n",
            "Epoch 4/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.3504 - loss: 1.1434 - val_accuracy: 0.3939 - val_loss: 1.0796\n",
            "Epoch 5/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.3328 - loss: 1.1202 - val_accuracy: 0.5758 - val_loss: 1.0905\n",
            "Epoch 6/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.4624 - loss: 1.0150 - val_accuracy: 0.3333 - val_loss: 1.1688\n",
            "Epoch 7/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.4605 - loss: 1.0258 - val_accuracy: 0.2424 - val_loss: 1.1954\n",
            "Epoch 8/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.4841 - loss: 1.0074 - val_accuracy: 0.4848 - val_loss: 1.2141\n",
            "Epoch 9/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.4956 - loss: 0.9455 - val_accuracy: 0.5152 - val_loss: 1.1078\n",
            "Epoch 10/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.5412 - loss: 1.0116 - val_accuracy: 0.4848 - val_loss: 1.1053\n",
            "Epoch 11/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.5539 - loss: 0.8583 - val_accuracy: 0.5152 - val_loss: 1.2383\n",
            "Epoch 12/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.4889 - loss: 1.0050 - val_accuracy: 0.5152 - val_loss: 1.2330\n",
            "Epoch 13/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.6574 - loss: 0.7688 - val_accuracy: 0.6364 - val_loss: 1.2809\n",
            "Epoch 14/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2s/step - accuracy: 0.5934 - loss: 0.8086 - val_accuracy: 0.4848 - val_loss: 1.6482\n",
            "Epoch 15/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.6696 - loss: 0.8167 - val_accuracy: 0.5758 - val_loss: 1.3190\n",
            "Epoch 16/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2s/step - accuracy: 0.6354 - loss: 0.7411 - val_accuracy: 0.6061 - val_loss: 1.3511\n",
            "Epoch 17/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.6785 - loss: 0.7209 - val_accuracy: 0.6364 - val_loss: 1.7026\n",
            "Epoch 18/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.6762 - loss: 0.6942 - val_accuracy: 0.5758 - val_loss: 1.7056\n",
            "Epoch 19/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.7190 - loss: 0.6478 - val_accuracy: 0.3939 - val_loss: 1.7090\n",
            "Epoch 20/20\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2s/step - accuracy: 0.7145 - loss: 0.5667 - val_accuracy: 0.5152 - val_loss: 1.4410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "5YzCJYH7cWRS",
        "outputId": "f82f3d5b-40db-429c-ffdd-7b7acc964df4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAf2NJREFUeJzt3Xd4lFX68PHvTHovJKSQhITeQ0dABQUEbFhBFFFE3XV1X3fRXeXnWlZd3V1d113XVdelqNgVy4qCgKLSpITeSyAJpIf0PvO8f5zMJIG0SWbmmUnuz3XlypOZp5zJZDL3nHPu+xg0TdMQQgghhNCJUe8GCCGEEKJrk2BECCGEELqSYEQIIYQQupJgRAghhBC6kmBECCGEELqSYEQIIYQQupJgRAghhBC6kmBECCGEELry1LsBbWE2mzl79ixBQUEYDAa9myOEEEKINtA0jZKSEmJjYzEam+//cItg5OzZs8THx+vdDCGEEEK0Q3p6OnFxcc3e7xbBSFBQEKAeTHBwsM6tEUIIIURbFBcXEx8fb30fb45bBCOWoZng4GAJRoQQQgg309oUC5nAKoQQQghdSTAihBBCCF1JMCKEEEIIXbnFnJG2MJlM1NTU6N0MYQMPDw88PT0lXVsIIbq4ThGMlJaWkpGRgaZpejdF2Mjf35+YmBi8vb31booQQgiduH0wYjKZyMjIwN/fn8jISPmU7SY0TaO6uprc3FxSU1Pp27dviwVxhBBCdF5uH4zU1NSgaRqRkZH4+fnp3RxhAz8/P7y8vDh9+jTV1dX4+vrq3SQhhBA66DQfRaVHxD1Jb4gQQgh5JxBCCCGEriQYEUIIIYSuJBjpRBITE3n55Zf1boYQQghhEwlGdGAwGFr8euqpp9p13u3bt3PvvffapY3vv/8+Hh4e3H///XY5nxBCCNEcCUZ0kJmZaf16+eWXCQ4ObnTbww8/bN1X0zRqa2vbdN7IyEj8/f3t0sYlS5bw+9//nvfff5/Kykq7nFMIIYTtsooqefX745RUdt7Cnp0uGNE0jfLqWl2+2lp0LTo62voVEhKCwWCw/nz48GGCgoL45ptvGDVqFD4+PmzcuJETJ04wa9YsoqKiCAwMZMyYMaxbt67Rec8fpjEYDPz3v//l+uuvx9/fn759+/Lll1+22r7U1FQ2b97Mo48+Sr9+/Vi5cuUF+yxdupTBgwfj4+NDTEwMDzzwgPW+wsJCfvGLXxAVFYWvry9Dhgzhq6++atPvRgghRD1N07j/vRReWHOEf313XO/mOIzb1xk5X0WNiUFPrNHl2gefno6/t31+pY8++igvvvgivXr1IiwsjPT0dK688kr+9Kc/4ePjw9tvv80111zDkSNHSEhIaPY8f/zjH/nrX//KCy+8wCuvvMJtt93G6dOnCQ8Pb/aYZcuWcdVVVxESEsK8efNYsmQJt956q/X+1157jUWLFvHnP/+ZmTNnUlRUxKZNmwAwm83MnDmTkpISVqxYQe/evTl48CAeHh52+b0IIURX8tXeTHaePgfAqn2ZPDpzQKcsZdHpgpHO4umnn2batGnWn8PDw0lOTrb+/Mwzz/DZZ5/x5ZdfNuqVON+dd97J3LlzAXjuuef45z//ybZt25gxY0aT+5vNZpYvX84rr7wCwC233MJDDz1EamoqSUlJADz77LM89NBDPPjgg9bjxowZA8C6devYtm0bhw4dol+/fgD06tWrPb8CIYTo0iprTPz5m8PWnzPOVXDgbDFDeoTo2CrH6HTBiJ+XBwefnq7bte1l9OjRjX4uLS3lqaeeYtWqVWRmZlJbW0tFRQVpaWktnmfYsGHW7YCAAIKDg8nJyWl2/7Vr11JWVsaVV14JQEREBNOmTWPp0qU888wz5OTkcPbsWaZMmdLk8bt37yYuLs4aiAghhGifJRtTOVNYQUyILwOig/j+SC7f7M+UYMQdGAwGuw2V6CkgIKDRzw8//DBr167lxRdfpE+fPvj5+XHTTTdRXV3d4nm8vLwa/WwwGDCbzc3uv2TJEgoKChqV1jebzezdu5c//vGPrZbcl5L8QgjRcTnFatIqwKMzBwCoYGRfFg9f0b/TDdW4/7t2F7Fp0ybuvPNOrr/+ekD1lJw6dcqu18jPz+eLL77ggw8+YPDgwdbbTSYTF198Md9++y0zZswgMTGR9evXc9lll11wjmHDhpGRkcHRo0eld0QIIdrphTVHKK82MSIhlGuTYymtqsXb08jJvDKOZpfSPzpI7ybalQQjbqJv376sXLmSa665BoPBwOOPP95iD0d7vPPOO3Tr1o3Zs2dfEHVfeeWVLFmyhBkzZvDUU0/xy1/+ku7du1snq27atIlf//rXTJo0iUsvvZQbb7yRl156iT59+nD48GEMBkOz81SEEELU23+miE9SMgB4/OpBGAwGgny9uLRvBOsO5fDN/sxOF4x0utTezuqll14iLCyMCRMmcM011zB9+nRGjhxp12ssXbqU66+/vsnuvxtvvJEvv/ySvLw87rjjDl5++WX+/e9/M3jwYK6++mqOHTtm3ffTTz9lzJgxzJ07l0GDBvH73/8ek8lk17YKIURnpGkaT391EE2DWcNjGZkQZr1vxpAYAFbvz9KreQ5j0NpaHENHxcXFhISEUFRURHBwcKP7KisrrZkesgS9+5HnTwgh6n2zL5P73k3B18vIdw9NJja0fh5eUXkNo55dS61Z47uHJtErMlDHlrZNS+/fDUnPiBBCCOECKmtMPPfNIQDuvbR3o0AEIMTfiwl9IgD4ppP1jkgwIoQQQriAZZtOkV5QQVSwD7+c1HR9pplDooHON1QjwYgQQgihs9ySKmsq7++nD2i2RMUVg6IwGmDfmSLSC8qd2USHkmBECCGE0NlLa49QWlXLsLgQrh/Ro9n9ugX6MC6pG9C5ekckGBFCCCF0dPBsMR9sTwfgiasHYTS2XNBs5lA1VPPN/kyHt81ZJBgRQgghdKJpGs/UpfJeNSyG0YnNL2JqMX2wCkZS0grJKqp0dBOdQoIRIYQQQidrD2az5WQ+3p5GHp0xoE3HRAX7Mrqnqj+yupP0jkgwIoQQQuigqtbEn75Wqbz3XJJEfLh/m4+dMcQyVNM55o1IMOLGJk+ezG9+8xu9myGEEKId3t58mtP55UQG+XDf5D42HWsJRrafKiC3pMoRzXMqCUZ0cM011zS7TstPP/2EwWBg7969drteRUUF4eHhREREUFXl/n+0Qgjh7vJLq/jnd2oZjd9d0Z9AH9uWiosL82dYXAhmDb496P69IxKM6GDhwoWsXbuWjIyMC+5btmwZo0ePZtiwYXa73qeffsrgwYMZMGAAn3/+ud3OK4QQon3+vu4oJZW1DI4N5sZRce06x8xOtFaNBCM6uPrqq4mMjGT58uWNbi8tLeXjjz9m4cKF5OfnM3fuXHr06IG/vz9Dhw7l/fffb9f1lixZwrx585g3bx5Lliy54P4DBw5w9dVXExwcTFBQEJdccgknTpyw3r906VIGDx6Mj48PMTExPPDAA+1qhxBCCDiSVcJ7P6cBalVej1ZSeZtjqca6+UQ+58qq7dY+PXS+YETToLpMn682rjno6enJ/PnzWb58OQ3XKfz4448xmUzMnTuXyspKRo0axapVq9i/fz/33nsvt99+O9u2bbPp13HixAm2bNnC7NmzmT17Nj/99BOnT5+23n/mzBkuvfRSfHx8+O6779i5cyd33XUXtbW1ALz22mvcf//93Hvvvezbt48vv/ySPn1sG9sUQgihaJrGs6sOYtZUMHFRr27tPldiRAADooMwmTXWHsq2Yyudz7ZBqjqvvvoqL7zwAllZWSQnJ/PKK68wduzYJvedPHkyP/zwwwW3X3nllaxatao9l29ZTTk8F2v/87bF/50F74A27XrXXXfxwgsv8MMPPzB58mRADdHceOONhISEEBISwsMPP2zd/9e//jVr1qzho48+avZ33ZSlS5cyc+ZMwsJUGtj06dNZtmwZTz31FKCey5CQED744AO8vLwA6Nevn/X4Z599loceeogHH3zQetuYMWPafH0hhBD1vj+Sw0/H8vD2MLJ45sAOn+/KoTEcziph9f4sZo+Ot0ML9WFzz8iHH37IokWLePLJJ0lJSSE5OZnp06eTk5PT5P4rV64kMzPT+rV//348PDy4+eabO9x4dzZgwAAmTJjA0qVLATh+/Dg//fQTCxcuBMBkMvHMM88wdOhQwsPDCQwMZM2aNaSlpbX5GiaTibfeeot58+ZZb5s3bx7Lly/HbDYDsHv3bi655BJrINJQTk4OZ8+eZcqUKR15qEIIIYAak5lnv1KpvAsuTiShW9tTeZtjGarZeCyPksqaDp9PLzb3jLz00kvcc889LFiwAIDXX3+dVatWsXTpUh599NEL9g8Pb1xN7oMPPsDf399xwYiXv+qh0IOXbX9YCxcu5Ne//jWvvvoqy5Yto3fv3kyaNAmAF154gX/84x+8/PLLDB06lICAAH7zm99QXd32ccE1a9Zw5swZ5syZ0+h2k8nE+vXrmTZtGn5+fs0cTYv3CSGEsM07W05zMq+MiEBvHrjMPsPdfaOC6B0ZwIncMr47nMOs4c2va+PKbOoZqa6uZufOnUydOrX+BEYjU6dOZcuWLW06x5IlS7jlllsICGjbcIbNDAY1VKLHl8G2SUizZ8/GaDTy3nvv8fbbb3PXXXdhqDvHpk2bmDVrFvPmzSM5OZlevXpx9OhRm85v+V3v3r270dctt9xincg6bNgwfvrpJ2pqLoyog4KCSExMZP369TZdVwghRGPnyqr5x3qVyvvQFf0J8r2wN7q9LFk1X+9z32qsNgUjeXl5mEwmoqKiGt0eFRVFVlbrqUXbtm1j//793H333S3uV1VVRXFxcaOvzigwMJA5c+awePFiMjMzufPOO6339e3bl7Vr17J582YOHTrEL37xC7Kz2z5BKTc3l//973/ccccdDBkypNHX/Pnz+fzzzykoKOCBBx6guLiYW265hR07dnDs2DHeeecdjhw5AsBTTz3F3/72N/75z39y7NgxUlJSeOWVV+z9qxBCiE7tH+uPUVRRw4DoILvP7bAsnPfD0VzKq2vtem5ncWo2zZIlSxg6dGirEzCff/556yTOkJAQ4uPdd1JOaxYuXMi5c+eYPn06sbH1E2//8Ic/MHLkSKZPn87kyZOJjo7muuuua/N53377bQICApqc7zFlyhT8/PxYsWIF3bp147vvvqO0tJRJkyYxatQo3nzzTesckjvuuIOXX36Zf//73wwePJirr76aY8eOdfhxCyFEV3E8p4R3tqosxic6kMrbnEExwSSE+1NZY2bDkVy7nttZDJrWxnxU1DCNv78/n3zySaM3xjvuuIPCwkK++OKLZo8tKysjNjaWp59+ulFmRlOqqqoaVQotLi4mPj6eoqIigoODG+1bWVlJamoqSUlJ+Pr6tvWhCBchz58QorO7c9k2NhzJZdqgKN6cP9oh13j+60O88eNJrh4Ww79uHemQa7RHcXExISEhTb5/N2RTz4i3tzejRo1qNIfAbDazfv16xo8f3+KxH3/8MVVVVY0yO5rj4+NDcHBwoy8hhBDC3Ww4ksOGI7l4eRj4vys7nsrbnJlD1byR7w/nUFljcth1HMXmYZpFixbx5ptv8tZbb3Ho0CHuu+8+ysrKrNk18+fPZ/HixRcct2TJEq677jq6dWt/gRchhBDCXdSazDy7SqXy3jkhkaQIByVuAMlxIcSG+FJWbeKnY3kOu46j2JzaO2fOHHJzc3niiSfIyspi+PDhrF692jqpNS0tDaOxcYxz5MgRNm7cyLfffmufVgshhBAu7r1taRzPKSU8wJsHLu/r0GsZDAamD4lm2aZTfLM/k2mDolo/yIW0qwLrAw880Oz6JBs2bLjgtv79+2PD1BQhhBDCrRWV1/DSWlWO4bfT+hHiZ79U3ubMHBLDsk2nWHswm+paM96e7rPii/u0VAghhHAT//zuGIXlNfSLCmTuGOdkhI7qGUZkkA8llbVsPuFeQzWdJhiRnhf3JM+bEKKzOZlbylubTwFqVV5PD+e81XoYDUwfrIZnVu9vvfaXK3H7YMTDwwPApjLpwnWUl5cDNLk2jhBCuKPnvj5ErVnj8gHduaRvpFOvbanGuuZAFrUms1Ov3RHtmjPiSjw9PfH39yc3NxcvL68LJs8K16RpGuXl5eTk5BAaGmoNKoUQwp1tPJbHukM5eBodm8rbnHFJ4YT5e3GuvIZtqQVM6BPh9Da0h9sHIwaDgZiYGFJTUzl9+rTezRE2Cg0NJTo6Wu9mCCFEh9WazDzz1UEAbh/fkz7dA53eBk8PI1cMiubDHel8sz9LghFn8vb2pm/fvjJU42a8vLykR0QI0Wl8uCOdI9klhPp78eAUx6bytmTGUBWMrD6QxR+vHYzRzuXnHaFTBCOgVg+WcuJCCCH0UFxZw0vfqlTe30zpS6i/t25tmdg7giBfT3JLqtiZdo4xieG6taWtZIKFEEII0UGvfnec/LJqekcGcNtFPXVti7enkWkDVVbNN/vcI6tGghEhhBCiA3anF7J0UyoAf7h6EF5OSuVtyYwhai7e6v2ZblFCQf/fmBBCCOGm1h/KZu5/tlJj0risfySX9e+ud5MAuLRfJP7eHpwtqmRPRpHezWmVBCNCCCFEO6zYepp73t5BRY2JS/tF8sqtI/VukpWvlweXD1CB0Tf7M3VuTeskGBFCCCFsYDZr/GX1Yf7w+X7MGswZHc+SO0YT6ONaOSGWAmir92e5/FCNa/3mhBBCCBdWVWvi95/s5YvdZwFYNK0fv768DwaD66XPTu4fiY+nkdP55RzKLGFQbLDeTWqW9IwIIYTQVXFlDUs2pvLkF/vJLKrQuznNKqqo4Y6l2/hi91k8jQZevDmZ/zelr0sGIgABPp5M6qfK0bv6UI30jAghhNDF6fwylm8+xcc7MiitqgVgZcoZHrtqIHPGxLvUm/yZwgruXLqNYzmlBPp48tq8kU5fd6Y9rhwaw7cHs/lmfxYPXdFf7+Y0S4IRIYQQTqNpGj+nFrB0YyprD2VjmcrQp3sgAd4e7Mko4tGV+/hqbybP3zCU+HB/fRsMHDhbxIJl28kpqSIq2Idld4516SGPhi4f2B0vDwPHc0o5ll1C36ggvZvUJAlGhBBCOFxVrYmv9mSydFMqB84WW2+f1C+Suy5O4tK+EZg1WLYplRfWHGHj8Tymv/wjj84cwLxxPXUraf7D0Vx+tWInZdUm+kcFsWzBGGJD/XRpS3sE+3pxcZ8Ivj+Syzf7s1w2GDForj7FFiguLiYkJISioiKCg90jGhVCCAF5pVW893Ma72w9TW5JFQC+XkZuGBnHggmJTb45puaV8cgne9l2qgBQK9H+5cZhJEYEOLXtH21PZ/Fn+zCZNSb26cZr80YR7Ovl1DbYw0c70vn9J3sZGBPMNw9e4tRrt/X9W4IRIYQQdnc4q5ilG1P5fPdZqmvNAEQF+zB/fCK3jk0gLKDltVvMZo13tp7mL6sPU15twtfLyMNX9GfBxCQ8HNxLomkaf193jH+uPwbADSN68Ocbh+Ht6Z45H+fKqhn9p3WYzBo//G4yPbs5L6iTYEQIIYRTmc0aG47msGRjKpuO51tvT44L4a6Lk7hyaIzNpdLTC8p55NO9bD6hzjcyIZS/3pRMn+6Bdm27RY3JzOKV+/hkZwYAv768D4um9XOpybTtMe+/P7PxeB6PzhzALyf1dtp1JRgRQgjhFGVVtXyaksGyTadIzSsDwGhQ66MsvDiJkQlhHXoz1zSN97el89zXhyitqsXb08hvp/bjnkuS8LTjOjAllTX86t0UfjqWh4fRwLPXDWHu2AS7nV9PK7ae5g+f7yc5LoQvHrjYadeVYEQIIYRDnSms4O3Np3h/WxrFlSo1N8jXk7ljE5g/vidxYfbNhDlbWMHilfv44WguAMPiQvjrTcMYEN3x94WsokruXLaNw1kl+Ht78OqtI7lsgGusM2MPuSVVjH1uHZoGmx69nB5OmoQrwYgQQgiHSEk7x5KNqazen4XJrN5CErv5s2BiEjeOinNoWXRN0/hkZwbPfHWQ4spavDwMPHBZX351We92r5Z7OKuYBcu2k1lUSUSgD8vuHMPQuBA7t1x/s9/YwrbUAh6/ehALL05yyjXb+v4tqb1CCNFJpBeUk3HOcRVMM4sqeHvLaXanF1pvm9C7G3dNTOLyAd2dkn5rMBi4eXQ8l/aL5LHP9rPuUDZ/X3eU1QeyeOGmYQzpYVsQsfl4Hr94ZyclVbX0jgxg+YKxLlHbxBFmDolmW2oB3+zLdFow0lbSMyKEEJ3AypQMHvp4D874j+7tYWTW8FgWTEzStfiXpml8uecsT355gMLyGjyMBu6b1JtfT+mDj6dHq8evTMngkU/3UmPSGJsUzpu3jybE3/1Sd9sqs6iC8c9/h8EAPy+eQvdgX4dfU3pGhBCii/j+cA6/+2Qvmgbx4X5teiNuDy8PI9MHR3HbuJ5EBvk45Bq2MBgMzBregwm9I3jyy/18vS+Lf31/nDUHsnjh5mSGx4c2eZymabz6/XFe/PYoAFcPi+HFm5Px9XLM781VxIT4MSIhlF1phaw5kMXt4xP1bpKVBCNCCOHGdp4+x33v7sRk1rh+RA/+dnOybtVK9RIZ5MO/bxvF1/syeeKL/RzLKeWGf2/inkt68dtp/RoFGbUmM49/sZ/3t6UD8ItLe/HIjAFd5nc2c0g0u9IK+Wa/awUj7lnBRQghBEezS7hr+XYqa8xM7h/JX28a1mXeVJty5dAYvv3tJGYNj8WswRs/nuTKf/zEjrpKrmVVtdzz9g7e35aO0QBPzxrM4isHdqnf2cwhMQBsPZlPfmmVzq2pJ8GIEEK4oTOFFcxfso2iihpGJITy79tGtjubpDMJD/DmH7eM4M35o+ke5MPJvDJufmMLT36xnzn/2cL3R3Lx9TLy+rxRzHehngFniQ/3Z0iPYMwarD2YrXdzrOQvVwgh3ExBWTXzl/xMVnElfboHsvSOMfh7y6h7Q9MGRbH2t5O4aVQcmgZvbTnN/jPFdAvw5v17LuKKwdF6N1E3lt6Rb/Zn6dySehKMCCGEGymvruWu5ds5kVtGTIgvb981ttV1XrqqEH8vXrw5meULxhAf7ke/qEA+vW8CIxLC9G6armYMUYHYpuN5FJXX6NwaRUJpIYRwE9W1Zn65IoXd6YWE+nvxzsKxbrWcvV4m9+/Oj7+7DE2jS80PaU7vyED6RQVyNLuUdYeyuXFUnN5Nkp4RIYRwB2azxu8+2cOPR3Px8/Jg6Z1j6NM9SO9muQ2DwSCBSAOuNlQjwYgQQrg4TdN4dtUhvth9Fk+jgX/PG8nILj7UIDpm5lA1VPPjsVxKq2p1bo0EI0II4fJe++EESzelAvDizclc1r/zLOAm9NE/KoikiACqa818dzhH7+ZIMCKEEE6TewTO7rbpkI+2p/PX1UcA+MNVA7luRA8HNKwLyNwDOYf0boXLMBgMzKybyLp6f6bOrZFgRAghnKOmApZOh6UzoDS3TYesPZjNoyv3AvDLSb25+5Jejmxh56RpsPHv8Mal8MYkKDipd4tchmXeyPeHc6moNunaFglGhBDCGU5thIpzUFsBaVta3X1bagEPvJeCWYPZo+N4ZEZ/JzSykzHVwP8ehHVP1f1cBd8+rmuTXMmQHsHEhflRUWPih6P6DtVIMCKEEM5w7Nv67fSfW9z1UGYxC9/aTlWtmakDo3ju+qEYDJIJYpPKYnhvDqS8BQYjTHwQDB5w+Cs48b3erXMJBoOBGXXF3/TOqpFgRAghHE3TGgcjaVub3TW9oJw7lm6jpLKWMYlh/OvWEXhKmXfbFGWo4bAT68HLH+a8C9OehrH3qPtXLwaT/hkkrmDmUDVU892hHKpq9Ruqkb9wIYRwtPzjcO6U+oQOajJldfkFu+WVVjF/6TZySqroHxXEf+eP6fTL2ttd5h7471TIOQCBUbDgaxhwpbpv8qPgFw65h2DnMn3b6SJGxIcSFexDSVUtm47n6dYOCUaEEMLRLL0iiZdAUAyYa+BsSqNdSqtqWbBsO6l5ZfQI9ePthWMJ8ffSobFu7Oi3sHQmlGRC5EC4ex3Ejqi/3y8MLn9MbX/3LJQX6NNOF2I0Gnjgsj48M2sww+JC9WuHblcWQoiuwhKM9JsO8ePUdoOhmqpaE794Zwf7zhQRHuDNOwvHEhXsq0ND3di2N+H9OVBTBr0mw8I1EJpw4X4j74Tug6GyEDY87+RGuqbbxydy+/hEIgJ9dGuDBCNCCOFIVaVwapPa7nsFJFyktusmsZrMGos+2sOm4/kEeHuwfMEYekUG6tRYN2Q2w5rH4OuHQTPDiHlw2yfgG9L0/h6eMKMuCNm+BLIPOq+tolkSjAghhCOl/qCGZcISoVufRsGIZjbxx/8dYNXeTLw8DLxx+2hdu8rdTnU5fDwftvxL/Xz5H+Daf4FHK8NbvSbBwGtAM8GaxWqCsdCVBCNCCOFIliGavleAwQBRQ8ErACqLePd/3/L2ltMYDPDS7OFc3DdC37a6k9JceOsaOPQ/8PCGG5fApb9Tv+O2mPYMePjAyQ1w5BuHNlW0ToIRIYRwFE2DY2vVdt8r1HcPT4gbBcDBbeq+p64ZzDXJsXq00D3lHoX/ToEzO9Sk1PlfwNCbbDtHeBJMeEBtr/k/qK2yfztFm0kwIoQQjpJzEIrPgKcvJF5svfm4zxAARhmP8P8u78MdExJ1aqAbOrURlkyFwtMQlgQL10HPCe0718WLIDAazqXC1tfs205hEwlGhBDCUSxDNEmXgpcfAJtP5PHcfjW5crLfSX47rZ9erXM/ez6Et6+DyiKIG6tSdyP6tP98PoEw7Y9q+8cXoCTbLs0UtpNgRAghHOW8IZr9Z4q49+2dbKvtjRkj3arPYiiVN8BWaRps+At8dq+aDDzoOrjjSwiwwxybobOhx2ioLoX1T3f8fKJdJBgRQghHqCisryXSZypF5TXctXw7pVW1DOkVB90HqftaKA0vgNpq+PxXsOE59fPE38BNy6w9TR1mNMLMv6jt3SvgzE77nFfYRIIRIYRwhJPfq9TRiH4QnsSzqw6SU1JFr8gA/jN/NMaejeuNiCZUFMKKG2DPe2qRu6tfVsMqRju/dcWNhuS5avubRyXVVwcSjAghhCM0GKL58WguH+/MwGCAF24aRrCvF8TXBSPSM9K0c6dhyRVw6ifwDoRbP4LRCxx3vSlPqpTrjG2w7xPHXUc0SYIRIYSwN7PZGoxUJF7O4pX7ALhjfCKjeoarfRLqysJn7W1y0bwuLWOnSt3NOwJBsXDXaug71bHXDI6BSx9S22ufgOoyx15PNCLBiBBC2FvWHijLAe9AXjwUzpnCCuLC/Pjd9P71+4TEQ3APMNfKPIWGDn0Fy6+CslyIHgr3rFffneGi+yG0J5SchY0vO+eaApBgRAgh7K+uV+Rc9ASW/nwWgOdvGEqAj2f9PgZDk4vmdVmaBlv+DR/Og9oK6DMNFnwDwU4sBuflC9P/pLY3/1MNFQmnkGBECCHsra6+yNLsvmgazB4dxyV9Iy/cz7pOTRcPRswm+OYRtU4MGoy+C+Z+AD5Bzm/LgKsh8RKorVTDNcIp2hWMvPrqqyQmJuLr68u4cePYtm1bi/sXFhZy//33ExMTg4+PD/369ePrr79uV4OFEMKlleVDxg4APi4aQGSQD49dOajpfS09I+nb1TyTrqi6DD64Dba9oX6e9gxc9ZIqm68HgwFm/BkMRjj4uar4KhzO5mDkww8/ZNGiRTz55JOkpKSQnJzM9OnTycnJaXL/6upqpk2bxqlTp/jkk084cuQIb775Jj169Ohw44UQwuWcWA9oHDInkEU3nr1uCCH+zawiGzVEZXBUFUHuIac20yWUZMGyK+HoN6pk/s1vwcT/1/bF7hwlegiMqsvc+eZR1XMjHMrmYOSll17innvuYcGCBQwaNIjXX38df39/li5d2uT+S5cupaCggM8//5yJEyeSmJjIpEmTSE5O7nDjhRDC1ZiPrgHge/Nwrhoaw/TB0c3v7OEJ8WPUdlebN5J9EP47FTJ3g38E3PEVDL5O71bVu+wx8A2B7H2Q8rberen0bApGqqur2blzJ1On1qdYGY1Gpk6dypYtW5o85ssvv2T8+PHcf//9REVFMWTIEJ577jlMpuYjzaqqKoqLixt9CSGEyzObqD6sJq9u9xrNU9cObv2Yrlhv5MT3sHQ6FKVDtz5w99r6oMxVBHSDyf+ntr97RhVgEw5jUzCSl5eHyWQiKiqq0e1RUVFkZWU1eczJkyf55JNPMJlMfP311zz++OP87W9/49lnn232Os8//zwhISHWr/j4eFuaKYQQusjY/xO+tUUUa/5ce9UsIoN8Wj/IUm+kq0xiTXkH3r0JqoohYQIsXAvhvfRuVdPGLISI/lCeDz/8Ve/WdGoOz6Yxm810796d//znP4waNYo5c+bw2GOP8frrrzd7zOLFiykqKrJ+paenO7qZQgjRISazxpY1HwBwKGAM143q2bYD48aoyZKFaVCc6cAW6kzTYP0z8OUDqrbK0Jth/ufgH653y5rn4QUz6tbE2fYG5B7Vtz2dmE3BSEREBB4eHmRnN15lMjs7m+jopsdFY2Ji6NevHx4eHtbbBg4cSFZWFtXV1U0e4+PjQ3BwcKMvIYRwZe9sOcWAEjVc3e/iGzC0dRKmTxBE1Q3ndNbekdoq+PRu+OlF9fOlv4cb3gTPNvQc6a3PVOg3UwVQa/5P79Z0WjYFI97e3owaNYr169dbbzObzaxfv57x48c3eczEiRM5fvw45gZpa0ePHiUmJgZvb+92NlsIIVxHekE5S9dsZajxFABhw6607QQJdf8/0zrhonnlBfD2dbD/EzB6wqxX4fLH9M+YscX0P4HRC46vhaPf6t2aTsnmYZpFixbx5ptv8tZbb3Ho0CHuu+8+ysrKWLBApUHNnz+fxYsXW/e/7777KCgo4MEHH+To0aOsWrWK5557jvvvv99+j0IIIXSiaRr/99k+xplS1M+xIyCwu20nsVZibToRwG3ln1AZM2mbwScY5n0KI+bp3SrbdesNF92nttcshtqme/VF+9lcVWbOnDnk5ubyxBNPkJWVxfDhw1m9erV1UmtaWhrGBss7x8fHs2bNGn77298ybNgwevTowYMPPsgjjzxiv0chhBA6+XhnBj8dy+N17z0AGPpeYftJLJVYs/ZBVSn4BNqxhTpJ+xk+mKsmf4bEw20fQ/eBereq/S79Hex5H/KPw7b/wIQH9G5Rp2LQNE3TuxGtKS4uJiQkhKKiIpk/IoRwGTnFlUx96QfKKys5EPArfEylcPd6iBtt+8leGgzFGTD/S+g1yf6NdaYDn8HKX4CpCmJHwNwPISiq9eNcXco7agKuTzD8OgUCmyjxLxpp6/u3rE0jhBDtoGkaj3+xn+LKWm7uflYFIv7d1Jtve1hTfN143oimqdVuP75TBSL9r4Q7V3WOQARg+G0QM1ylJX/3jN6t6VQkGBFCiHb4el8Waw5k42k08FBS3equfaaC0aPlA5tjncTqphk1plr46jew7kn187j7YM4K8A7QtVl2ZTTCzL+o7ZS3IXOPvu3pRCQYEUIIG50rq+bJL/cD8KvJvYnI/EHd0Z75IhbWRfO2ud9aKJXF8N5s2LkcMMCMv8DMP7c/MHNlCRfBkJsATa1b4/ozHdyCBCNCCGGjZ746SF5pNX27B3L/KF/IOagKl/W+vP0njRoM3kFQXaLO5y6KzsCymWqBQC9/uOU9uOiXerfKsab9ETz9VJbQgc/0bk2nIMGIEELY4PsjOazcdQaDAf5y0zB8UuvqLsWN6Vg1UaNH/cRXdxmqydwL/50C2fshoLuaHzLAxhor7igkDi7+rdpe+wRUl+vbnk5AghEhhGijksoaHlu5D4C7JiYxMiEMjqmF8eg7reMXsKT4usMk1qPfqh6RkkyIHAj3rIceI/VulfNM+LVKWS5Kh82v6N0atyfBiBBCtNFfVx/hbFElCeH+PHRFP1Xm/OQGdWdH5otYWIIRV6/Euv2/8P4cqC6FpElw12oITdC7Vc7l7Q/TnlbbG/8ORRn6tsfNSTAihBBt8PPJfN7ZqrJm/nzDUPy9PeH0Jqgph8BoiB7W8Yv0GA0GDyhKU3MxXI3ZDN/+AVY9BJoZhs+D2z4Bv1C9W6aPwderlYdrK2Dtk3q3xq1JMCL0UZIF6dv1boUQbVJZY+LRuuGZuWPjmdAnQt1hHaKZap+1VnwCIXqI2na1RfNqKuCTO+uHJC7/A8z6F3h24TXGDAaVNYRBrb1zupOV83ciCUaEPj64DZZMhZxDerdEiFb9fd1RUvPKiAr2YfGVDUqaH6tbNM0eQzQW8S46VPPji3DwC/Dwhhv+q8qju9Nid44Skwwj56tty6rEwmYSjAjnqy6Hs2pRMc7u1rUpQrRmb0Yhb/54EoA/XTeUYF8vdUf+CbVOidETek223wWtlVhdqGfEVAu73lHbs16FYTfr2x5XM/ou9T1jh9QdaScJRoTzZR9Q480ABSf0bYsQLaiuNfP7T/Zi1uCa5FimDmpQ1vz4OvU9YTz4htjvovENF80rsd95O+LYt1CaDf4RMOg6vVvjeroPVEFpZaHKrhE2k2BEOF9WgxLK+RKMCNf1+g8nOJxVQpi/F09dM6jxndYhGjuk9DYU0gNCElTAnrHDvudur10r1PfkW7r2HJHmePrUr0icuVfftrgpCUaE8zVcz0F6RoSLOpZdwivfHQPgqWsH0y3Qp/7O6nJI/Ult23O+iIUrLZpXkg1HV6vtEbfr2xZXFp2svst6Ne0iwYhwvoafHPJPyhircDkms8bvPtlLjUljyoDuXJsc23iHUz+pVWlD4iFygP0bYFmnxhUqse55HzSTqjDb3QGPtbOIqUvtzpKekfaQYEQ4l6mm8bob1SVQlqtfe4RowvLNp9idXkiQjyfPXj8Ew/lZIw2HaByRUWIpfpaxQ99F8zStfohGekVaFmPpGZFgpD0kGBHOlXsYTNXgE6w+VYLMGxEuJS2/nBfXHAFg8ZUDiQnxa7yDpjkmpbeh7oPUa6S6RE341kv6z5B/TC2AN/h6/drhDqKGAAYoOQul8gHLVhKMCOeyfGqIHgrdeqttmTciXISmaTy6ci8VNSbG9+rG3LHxF+6UdxQK01S9jaRLHdMQo4caFgF9h2os6byDrwffYP3a4Q58Auv/p2XJvBFbSTAinMsynhqTDOF1L1zpGREuoKi8hj9/c5jNJ/Lx9TLy5xuHXjg8A/W9IokXg3eA4xpkXTRPp2CkqgT2f6a2ZYimbWSopt089W6A6GIsM82jh0FFgdqWnhGhoxO5pSzfdIpPdmZQUaPmZzx8RX96dmsm0HD0EI2FdRKrThk1Bz6DmjLo1qc+MBItix4G+z+VjJp2kGBEOI/ZrAo5gZp5XlhXHCj/pH5tEl2SpmlsOp7P0k2pfHc4x3r7gOgg7r20F9eP6NH0gZXF9euPODoYiatbNK84Q60IGxLn2OudzzpxdZ6UfW8ryahpNwlGhPOcS1VLjnv6QkR/NeYOUFCX3iv/8ISDVdaY+GL3GZZuPMWRbFXd1GCAKQO6c9fFSYzv1a3poRmL1B/AXAPhvernBziKd4CaW5W5W80bGXqTY6/XUO4RNXnV4AHJc513XXdnqTVScFIFrjLPps0kGBHOY+m67D4IPDwhtCcYjKoruCQLgmP0bZ/otHKKK1mx9TQrfk6joKwaAH9vD2aPjueOCYkkRbRx7oezhmgsEsbrE4xYJq72vQKCop13XXcX0A2C41RvVtY+SJyod4vchgQjwnkswYilK9PTG0IT4NwpNW9EghFhZ/vPFLF0Uyr/23OWGpMqrtcj1I87JyQye0w8IX5ebT+ZpsGxtWrb3iXgm5MwDn5+zbmTWE01sOcDtT1SJq7aLGZYXTCyV4IRG0gwIpynYSaNRXhvFYzkn1DZCUJ0kMmsse5QNks2prIttcB6++ieYdx1cRJXDIrC06MdiYTZ+6EkEzz9oKeT/lYti+ZlH1DZLT5Bjr/m0TWqEGFAd+f1AHUmMclw5GvJqLGRBCPCOTStQY2RBsFIt95wYr1k1IgOK6ms4eMdGSzffIq0gnIAPI0GrhoWw4KJSQyPD+3YBSxDNL0mgZdvx87VVsExqvewMA0ytkPvyx1/TcsQTfIt4GFDz5FQout6fiWjxiYSjAjnKD4L5XlqQlxUg9VPpdaI6KD0gnKWbTrFRzvSKa2qBSDU34tbxyZw+/ieF1ZQbS9nD9FYxF+kgpG0nx0fjBRn1gddUlukfSzD0LmHoabSeYGrm5NgRDiHZYgmsj94NXhzsFZhlfRe0XaaprH91DmWbDzJ2oPZmOvWWuwdGcBdFydxw4g4/Lw97HfBinP1K+j2cXIwknAR7PsI0rY4/lp73gfNrAKgyH6Ov15nFNwD/LtBeb5ah6vHSL1b5BYkGBHOYR2iGdb49vBe6ntBqqpDYpSiwKJ5uSVV/Hg0l2WbU9l/pth6+yV9I1h4cRKX9o3EaHRAiviJ79SbdOQACOtp//O3pOGieaZalYnmCI0WxZvnmGt0BQaD+j938ns1VCPBSJtIMCKc4/xMGovQnmD0hNoKNTkwpJliU6LLqa41czCzmF1p59iVVkhK2jkyzlVY7/fxNHLDyDgWTEykX5SDJ3bqNUQDEDkQfEKgqkhNoo0d7pjrpG1Rc7e8A2VRvI6KqQtGpPhZm0kwIpyjqUwaqK83UnBCfUkw0iVpmkZmUSW70grZlXaOlLRz7D9bTHWtudF+BgP06x7ENckx3DquJ+EB3o5vnNncIBjRIbvEaIT4MXB8nRoqclQwktJgUTyfQMdco6uQNWpsJsGIcLzyAiiqK/0ePfTC+7v1VoFI/gnHrYIqXEpljYl9Z4pU4HG6kF3p58gurrpgvzB/L0YkhDEiPpSRPcMYFhdCkK+TMzwyd6nJ195B9am2zhZ/kQpG0rbCuF/Y//yVxXDwc7UtE1c7zpIxmL3fsUNrnYj8hoTjWYZowhLBN+TC+y0ZNZLe2ylpmkZaQbl1qGVXWiGHMouptcw6reNhNDAwJogR8WGM7BnKiPgwenbzb7k8uzNYekV6T1aF+vRgmTeSttUxSyccWAk15RDRD+LH2vfcXVF4LzXcVV0K+ceg+0C9W+TyJBgRjtfcEI2FJaNGFszrFMqqatmTXh947EovtJZgbygyyIeRCaGMTAhjREIYQ3uE2DcDxl6cXQK+KT1GqblVJWdVL2Nogn3PbxmikUXx7MNoVL3AaVvUUI0EI62SYEQ4XnOZNBbWjBrpGXFnx3NKWLLxFJ/tyqCypvFcD28PI4N7BNcFHqGMSAgjNsRX/16P1pTmwpkUte3slN6GvP3V6+dsiqo3Ys9gJOcQnNmhgh1ZFM9+oofVBSN7IHmO3q1xeRKMCMezZtK00jMi6b1uR9M0fjyWx5KNqfx4NNd6e49QP0b2VHM9RiSEMig2GB9PF+z1aM2J9YCmPuXqvXZSwkUqGEnfCsNutt95Lem8/WZAYHf7nbers2QOSkZNm0gwIhyrqhTyj6vt5oKRkHjw8AZTlVpgyt5d0MLuKqpNrNyVwbJNpzieUwqo3v0rBkVx18QkxiaFu36vR1u4whCNRfw42Ppv1TNiL7XVqtAZSG0Re2uYUeOIeT6djAQjwrGyDwAaBEY3/6nL6KEmt+YdVRk1Eoy4rKyiSt7ecor3tqVRWF4DQKCPJ7NHx3PnhEQSuvnr3EI7MtXC8fVq2xWCEcsk1uz9UFnU9GRwWx1drSqFBkbrOwzVGUUOUB+yqorUYqDhSXq3yKVJMCIcq7liZ+cL762CkYIT0Psyx7dL2GRvRiFLN6by1d5MaxZMfLgfd05IYvboOOen2zrDmR1QWQi+odBjtN6tgaBoFbSfO6UWzesztePntCyKN3yupJ/am4eXmriauUcN1Ugw0iL56xOOldXKfBELyahxObUmM2sPZrNkYyo7Tp+z3j42KZy7JiYxbVAUHo4ove4qLEM0faa4zht1/EUqGEn7uePBSPFZVbsEYLgM0ThETLIKRjL3wqBZerfGpbnIK0x0Wq1l0lhIRo3LKK6s4aPt6SzbdIozhar8upeHgauHxXLXxCSGxtlheMAduNJ8EYuEcbD3AzWJtaN2v6fW20mYABF9On4+cSHL/z1LD7FolgQjwnFqq1XaILQ+TGPtGZFgRC+n8spYvvkUH+9Ip6zaBKgKqPMu6sm8i3oSFdyFlkIvPgtZ+wAD9J6id2vqWSrAZuwEU40aCmgPs7k+i2akVFx1GEuPsGTUtEqCEeE4uYfAXKMm2oW2stKppQrruVNSPtmJNE1j68kClmxMZf3hbLS6oqj9ogK5a2IS143oga+XG6bkdpRl+KLHSAiM1LctDUUOUK+nyiIVLLV3RdjTm+BcqipxL8MHjhM1GAxGKM2Gkiw170c0Sf7jC8dpOETTWlpbcA/w9IXaSlVhUiZ7OVRVrYkvd59l6aZTHMostt5+Wf9I7ro4iYv7RHSO1Nz2csUhGqhbNG+cal/6z+0PRiy9IkNuAO8A+7VPNOYdAN36Qt4R9f9QgpFmSTAiHKe1YmcNGY0QlqR6UwpOSDDiIHmlVazYepoVW0+TV6pKtPt6GblxZBwLJibRp7us1kptNZzYoLb7umC6qyUYSdsKF91n+/GVRXDwC7U9cr592yYuFDNMBSNZe6CfiwW3LkSCEeE4ra1Jc75uvVUwkn8SZD6dXR3KLGbpxlS+2H2WapMq1R4d7Mv8CT25dWwCof46LQDnitK3QnUJ+EdAzAi9W3MhS72R9J/bV0xr3ydQW6GGfHqMsn/7RGMxybDv4/qeYtEkCUaEY5hNkLVfbbeWSWMhGTV2ZTZrfHc4h6WbUtl8It96e3J8KHdNTOTKoTF4eUjp/QtYh2imuebSBLEj6xbNy4TCNAhrZT7W+SxDNCNul6qgziAZNW0iwYhwjPwTUFMGnn4Q0bdtx0hGjV2UVdXyyc4Mlm1K5VR+OQBGA8wcEsNdFycxMiG0a88Hac2xteq7Kw7RgFo0LyYZzuxUQzW2BCPZB9T6NkZPSL7FcW0U9aKHqu+Fp6GiEPxC9WyNy5JgRDiGZYgmeogq994Wlowa6Rlpl4xz5by95TTvb0ujpLIWgCBfT24dm8Dt43sSF9aJSrU7yrnTkHtYZUD0vlzv1jQvYbwKRtK32rYibEpdxdX+MyEgwjFtE435h6slLgrTVAZU0iV6t8glSTAiHMPSJdnWIRqo7xk5d7pjNRS6EE3TSEk7x9KNp1h9IAtTXan2pIgAFkxM5MaRcQT4nPcyL81Rn9Ai+zm/wR1lNqs34Moix5w/9Uf1PX4c+IU55hr2ED8OtvzLtkXzaqtg74dqe4RMXHWq6GEqGMnc45rByJFvVOZPwnjd/u9KMCIco61r0jQUFANe/lBTrl64luBEXKDGZObrfZks3XSKPemF1tsn9unGXROTuKx/d4xNlWo/+QN8eDtUFcMVz8L4+91n3kB1GXx6DxxZ5fhrueoQjYVlEmvOwbZ3/R/5GioKIChWlbgXzhOTDIe/ct3iZ2ufUGuD3fwWDL5OlyZIMCLsT9Nsz6QB9aYY3kutSpp/QoKRJhSWV/PetjTe3nyarOJKALw9jVw3PJa7Lk5iQHRw8wfvfg++/DWY1RAO3z6mCl/N+IvrF5kryYb358DZXeDhUz8O7wj+3Vy/5yCwu0qFP5cKGTugbxvWqUlpsCheW4dOhX1Y/g+6YkZNQaoKRAweui5S6uL/gYRbKsqAinNqklz3QbYdawlGZN5II8dzSlm2KZVPUzKorFGpuRGBPtx+UU9uuyiBiECf5g/WNPj+Ofjxr+rnITeqf45rn4Tt/4XCdLhpKfi4aI2RnEPw7mwoSgO/cJj7gVqjpatLuEgFI2lbWg9GijLgxHdqe4Qsiud0luHqvCNQXa4mIbsKS7XhhPGquq9OJBgR9mcZookcAJ4tvEk2RTJqrDRN46djeSzdlMqGI7nW2wfFBHPXxUlckxyDj2crn3Brq1RviGWuwCUPwWV/UCmr4b3UsMexNbBsJtz6EQTHOPARtcPJDfDhfKgqUhOcb/tYeswsEi6CPe+reiOt2f0eoEHiJfUp9MJ5gqIhIBLKctXQWtxovVtUr2Equ44kGBH2154hGgvJqKGq1sTKlDMs3ZjKsZxSQI1gTR0YxcKLkxiXFN621NzyAvhwnlqHxOAB17zcuOLmwGvgzlVq+CNrL/x3inqzjxrsmAdmq13vwv/+nxpWShgPt7ynMhOEYl00b0fLE74bLoonvSL6MBjU/8Pj69SHNVcJRmoq6idt67z0gQQjwv4arkljqy7eM5JfWsXdb+9gV1ohAAHeHtw8Op4FExPp2c2GNUQKUuHdmyH/GPgEw+y3mk5VjRsFd69T++YdhSXT1b56TnDUNPj+T/DjC+rnITfBrFfBqwutGtwWEf3ANxQqC1Uw2Vw11VM/qRoXPsEw8FpntlA0FD2sPhhxFac2qvXAguOg+0Bdm+KC5QWF22tPJo2FpWekKF2tEdKFnMor48bXNrMrrZAQPy/+cNVAtvzfFJ66drBtgUj6dvjvVBWIBMfBXatbrpkRlggLv1Vd+NUlKjDZubyjD6d9aqtg5b31gcglD8MNb0og0hTLonnQcorvrrqJq0NudK25Cl2N5f+hK2XUNByi0Tmrrl3ByKuvvkpiYiK+vr6MGzeObdu2Nbvv8uXLMRgMjb58feUfS6dVlgclZ9V2ezIeAruDdyBoZjh3yq5Nc2Upaee44bXNnMovJy7Mj0/vm8Ddl/Qi2NfGnP+DX8BbV0N5nuoWvntd24Zd/MJg3koYdgtoJvjfg7DuKdXF7yzlBfDO9bDvIzX5+dp/wZTHXbMku6uwTORN29L0/RXn4OCXanvk7c5pk2iaZdg6+6AaVtObprnU6tQ2v8o//PBDFi1axJNPPklKSgrJyclMnz6dnJycZo8JDg4mMzPT+nX69OkONVq4MEuvSHhv8Amy/XhLei90mXkjaw5kMfc/Wykoq2ZojxBW/mqC7avnahps+id8dIfqdu03A+782rYJqZ7ecP3rMHmx+nnj3+HThVBTaVtb2qPgJCy5Qs1v8QmG2z6RN8+2SBivvlsWzTvfvk/AVAXdB6s1bYR+QhPV37apCnKP6N0ayD+uPvB5eEPSpXq3xvZg5KWXXuKee+5hwYIFDBo0iNdffx1/f3+WLl3a7DEGg4Ho6GjrV1RUVIcaLVxYR4ZoLLrQvJHlm1L55YqdVNWauax/JB/cexHdg2zsOTTVwqqHYO3jgAZj71WTPduTqmswwORH4brXwegFB1bC27OgLL/1Y9srfdt5w0prdK134FZiR6jnqTS76Z5EyxDNiHm6d8N3eUZjfW+xKwzVWHpFek50ibR+m4KR6upqdu7cydSp9TntRqORqVOnsmVLM92EQGlpKT179iQ+Pp5Zs2Zx4MCBFq9TVVVFcXFxoy/hJjqSSWPRBTJqzGaNP606yFP/O4imwdyxCbw5f/SFpdtbU1UCH8yFHUsAA0x/Hmb+teNFrYbPhds/U3UH0rfCkqmOCQ4PfA5vXQPl+epv5p71EGVjbZquzMsPYoer7fNTfDP3qg8HRi8YZsP6NcJxXKn4mQsN0YCNwUheXh4mk+mCno2oqCiysrKaPKZ///4sXbqUL774ghUrVmA2m5kwYQIZGRnNXuf5558nJCTE+hUfH29LM4WeOpJJY9HJe0Yqa0z8+oNdvPlTKgC/m96f564fgqeHjR2VxWdVfZBj36rVkee8A+N/Zb9PwEmXwMK1apGvgpOq9+J08x86bKJpsOkf8LFlWGmmGlYKirbP+buS+GbmjVjSeQdcBQHdnNsm0TTL/0W9M2qqSuHUJrXtjsFIe4wfP5758+czfPhwJk2axMqVK4mMjOSNN95o9pjFixdTVFRk/UpPT3d0M4U9VBbX92bYpWfkZMfb5GIKy6u5fcnPrNqbiZeHgb/PSeb+y/q0rW5IQ1n74M0p6ntApKoXMvAa+zc4sj/cvV6ljVYUwNvXqnkIHWGqhVWL1HoYAGN/Abe86xJdxW7Jsk5Nw4yamsoGi+LJ3BuXYc2o2efcyeHnS/0BzDVqSQEXKSJoU59wREQEHh4eZGdnN7o9Ozub6Oi2faLx8vJixIgRHD9+vNl9fHx88PGxsXKn0F/2fvU9uEfHlie3vDiKMtQ/1U6S1pleUM4dy7ZxMreMIB9P3rh9FBP6tOP3dGyd6lGoLoWI/nDbRyo911ECu8MdX8HKe9RiX58uVHUrLl5key9MVQl8vACOrwUMMON5uOg+hzS7y7AUP8s9pLJn/MLUYoKVhWoOjsy/cR0R/cHTV6XQn0vVLxBoOETjInOJbOoZ8fb2ZtSoUaxfv956m9lsZv369YwfP75N5zCZTOzbt4+YGBcrOy06zh5DNKAWKvMJATT1gu0E9mYUcv2/N3Myt4yYEF8+uW9C+wKRHcvgvdkqEEm8BBaucWwgYuHtD7PfhvEPqJ/XP63KzNuSolh8FpbOVIGIpx/MWSGBiD0ERtb3JqZvV9+ti+LdKoviuRKPBut16TVUo2lwbK3adpEhGmjHMM2iRYt48803eeuttzh06BD33XcfZWVlLFiwAID58+ezePFi6/5PP/003377LSdPniQlJYV58+Zx+vRp7r77bvs9CuEa7JFJAypS71aX3tsJ5o18dzibOW9sJa+0igHRQXz2q4n0j7Yx7dlsVsMaX/1G1QFJnqvqgviFOaTNTTJ6wPQ/wZUvgsGoMjXevRkqi1o/1jKslF03rLRgFQy82vFt7iosQzXpW6EwTa3pAyoYEa5F7+JnOQeh+Iz6QJA4UZ82NMHmcvBz5swhNzeXJ554gqysLIYPH87q1autk1rT0tIwNihSdO7cOe655x6ysrIICwtj1KhRbN68mUGDZMZ8p2OPTBqL8N5quXg3z6h57+c0/vD5PswaXNI3gn/fNpIgWwuZ1VTCZ7+Ag5+rnyf/H0z6vX7dq2PvgZB4+GQBnPwels5Qi+yFNjPR/Nha+PjOBsNKH0NYT6c2udOLHwe734W0rapuBJqqHRGepHfLxPn0zqixDNEkXaqysVxEu9ameeCBB3jggQeavG/Dhg2Nfv773//O3//+9/ZcRriTmkrIPay2OzpMA26fUaNpGi9+e4RXv1ftv2lUHM/fMBQvWzNmyvJV6m76zypFc9a/IPkWB7TYRv1nwIJv4L056pPWf6fCrR/Wp5la7FgKqx5WvTmJl6ihGb9QPVrcuVl6Rs7sVD0jACPmN7+/0E+0JRjZo4ZMnP2hwjpEo+8qveeTOsvCPnIOqtVV/cIgJK7j53PjjJrqWjO//XC3NRB5cEpfXrhpmO2BSP4JVd8j/WdV7+P2z1wjELGIHa7KzXcfBKVZsOxKOLJa3WcdVvrtecNKoXq2uPOK6Ad+4SpNuihdzbmSYTDXFDVIraJdngclmc69dkWh6j0DCUZEJ9VwiMYekb6b9owUVdRwx9JtfL77LB5GA3+9cRi/ndbP9tTd01vgv1NUMBbaU9X7SLrEMY3uiNB4tRBfr8ugpkz14mz5txrC2fQPtc/k/4PrXlPl5oVjGAz19UYAht3sUl3wogEvPxU8gvOHak5+rz4cRPR3zsR3G0gwIuzDXpk0Fpb1aUrOQnW5zYe/+/NpXlxzhHUHs8kvrbJPm1pxtrCCm1/fzJaT+QR4e7D0zjHMHtOOgn0nvlf1PCrOqfoed69X9T5clW+Imgcycr5a4HDNYjW/xegF178Bkx9xmfTBTi2hQTAitUVcW0yDoRpnctEhGmjnnBEhLmDNpLHD5FUA/3A15FNxTvUORA9p86GHs4p57LP9jW7r2c2fEfGhjEgIY0RCKANjgm0fNmnBwbPFLFi+jeziKroH+bBswRgGx4a072Q/vwGmarXY3U3L3GPZdw8vuOaf6tPW+qdVgDLnXdfszems+kxTv/seo+33OhSOETMM9n7g3Iwas9klU3otJBgRHWc2QXbdekP2/CcY3hvO7FAZNTYEI98fzgWgR6gfft4eHM8p5XR+Oafzy/l891kAfDyNDIsLUcFJXZASHdK+4mo/Hs3lV++mUFpVS7+oQJYtGEuP0A50kefXFQQc90v3CEQsDAa45CHofyX4R6j6F8J5oofAfVsgKEp6olydtSy8E4ORrD1QlgPegfWrPbsQCUZEx+Udg9oK8Aqon3hqD93qghEb541sOJIDwC8m9WL++ESKKmrYk15ISto5dqUVsivtHMWVtWw/dY7tp85Zj4sN8bX2nIxICGVwbAi+Xi0XjPp4RzqLV+6j1qxxUa9w3rh9NCF+NqbuNmSqrV991UXKNNus+0C9W9B1dR+gdwtEW1hW7y1Kg/IC1RPsaJZekV6TXXL+lgQjouMsQzTRQ9Qy2fbSjtV7Sypr2HlaBRiT+3UHIMTPi0v7RXJpP/VJ3WzWSM0vY1dafYByJKuYs0WVnN2Xyap9aoa7l4eBQbEhdT0noYxMCCMuzA+DwYCmafxj/TFeXncMgFnDY/nrTcPw8exgtcuidLVmhIePKuUthOh8/ELVkOa5U2qoptdkx1/TxVbpPZ8EI6Ljsuw8edXCmlHT9vTeTcfzqDVr9IoIIKFb00McRqOB3pGB9I4M5KZR6g2/rKqWvRlF7Eqv7z3JK61mT3ohe9ILWb5ZHRsR6MOIhFAA1h5UazT9anJvHr6iP0ajHbrGLYFXeJJ9AzshhGuJHqaCkUwnBCNl+ZCxQ2274ORVkGBE2IO9J69aWDJqbOgZ+eGomi9i6QVpqwAfT8b37sb43mqpdU3TyDhXUT+0k17IwbNF5JVWWYMQowGenjWEeRfZsZqoJfCy53CXEML1xCTDoS+dk1FzYj2gQdRQCI51/PXaQYIR0TGa1qDGiIN6Rkqz1WqvPi2v56JpGhuOqGBkcv+OTZ40GAzEh/sTH+7PrOE9AKisMXHgbBG70go5mFnMrOE9mGRj0NMqS+BlWZtHCNE5WT68OSOjxjpE45q9IiDBiOiowtNqoTSjF0TaeeKib4jKyijPU+m9rfS8HM0uJbOoEh9PIxf16mbftgC+Xh6M6hnOqJ4OnGxmmawrPSNCdG6WYe28Y1BdBt4BjrmO2QTH16ltF50vAlL0THSUJTWt+0DHzNC2oRKrJYtmfO9urWbBuCxrz4gEI0J0akFREBgNaJC1v9Xd2+3MTlWvyTcE4sY47jodJMGI6BjrfBE7D9FY2JBRYx2isffQibOYauDcabUtPSNCdH6W/5uOHKqxDNH0ngIerjsYIsGI6BhrJo2DKj5a5k60klFTWlXLjtMFAEzu390xbXG0wjS1boSnHwTF6N0aIYSjWYufOXASq4un9FpIMCI6xjJM46jy023sGdl0PI8ak0ZiN38SIxw09upo1vkivSStV4iuwNFr1JRk1Z+7z1THXMNO5D+eaL+SbLV0PAaIGuyYa7RxzohliMbu2S3OJJk0QnQtlmGanENQW23/81smrsaOdPnlGSQYEe1nGaLp1gd8Ah1zDUutkfI8lbXTBE3T+PGoJaXXTYdoQDJphOhqQnuqiaXmGsg9bP/zu8kQDUgwIjrCUcXOGvIJgsAotd1M78jxnFLOFFbg7aCUXqeRTBohuhaDwXHzRkw1cOJ7tS3BiOjUHFXs7HzWeSNNT2K1DNFc1Ksbft5umtIL0jMiRFfkqOJn6T9DVbGq1RQ7wr7ndgAJRkT7WRfIc3AwYs2oabpnZMNRVV/EbVN6QY0XF6WrbekZEaLrsPaM2DkYsQzR9JnqFhPiXb+FwjVVFtUvde/IYRpoMaOmrKqW7al1q/R2sAS8rs6dAs0M3oH1w1JCiM7P2jOyT1VLtZdja9V3Fy4B35AEI6J9svap7yHx4O/A8ujQYkbN5hP5VJvMJIT7k+SuKb3QeLVegx1W/xVCuIeIvqq2UE1Zs0PRNitMh5yDYDBC78vtc04Hk2DEnRWkQk2lPtd21hANtNgzYikBP7l/JAZ3fhOX+SJCdE1Gj/rSCPaaxHq8rlckbqzjPyzaiQQj7upMCvxzBLw3W62c62yOLnbWkCW9t+IclBdYb264Sq9b1xcByaQRoiuzd/EzNxuiAQlG3NfR1YAGqT/AgZXOv76zMmkAvP0hKFZtN+jGPJFbplJ6PYyM7+3GKb0gPSNCdGX2XKOmtgpOblDbbpDSayHBiLtK21q//e0TUF3uvGvXVEDuEbXtjGEaaHLeiGWIZlyvcPy9XXcBqDaxBFnSMyJE19Mwo6ajPd2nN0FNuVoROHpox9vmJBKMuCNTLWTsUNs+IVCcAZv/6bzrZx9UC7r5R0BwrHOuaRmqaTBv5IejnWSIpqYSijLUtvSMCNH1dB8ERk+oKKj/X9BeDYdo3GgenQQj7ih7v5p57RMC1/xd3bbxZTWD2hmyLJVXhznvj/28npHy6lp+Punmq/RanEsFNPAJhoAIvVsjhHA2L1+IHKC2OzpU40Yl4BuSYMQdpf+svsePgcE3QM+JUFsB6550zvWdmUljcV5GzZa6lN64MD96R7pxSi80Xq3XjT7JCCHsyB7Fz/JPQP5x1cvSa7JdmuUsEoy4o7Qt6nvCRerNa8afAQPs/xROb3b89Z2ZSWNh7Rk5CQ2yaNw+pRckk0YIYZ+MGssqvQnjwTe4421yIglG3I2mQZqlZ+Qi9T1mGIy6Q21/84h9q/idz1QD2QfqruvEYCQsCTBAVRFaWZ61BPykfm4+RAOSSSOEsE9GjZsO0YAEI+6nKB1KzqpuuB6j6m+//HE1hyRrL+x+13HXzzsKpirwDqoLEJzEyxdC4gA4c3I/6QUqpXeCu6f0gmTSCCEgaoj6XnwGyvJsP766HFJ/UtsSjAiHs/SKRA9T9TcsAiJg8iNqe/3Tau0YR7AM0UQPdf7iS3UZNScPqzaMSQojwMfNU3pBekaEEGpYxfI/oD1DNad+Uh8UQxIgsr992+YEEoy4m/S6+iIJF11435h7oFtfKMuFH19wzPWdWezsfHU9B+cyDgMwuTMM0VSXq54ukJ4RIbq6jgzVWIdo3Cul10KCEXeT1kIw4ukNM55X21tfh7zj9r++Hpk0FnWfGrwK1bCGW6/Sa2EZovENdZs1JIQQDtLejBpNc+v5IiDBiHupLKqfPBrfRDACKiruewWYa+Dbx+x7fbO5frVeZ05etajrOUggix6hfvTpHuj8NtibZNIIISzam1GTdxQK08DDB5IusX+7nECCEXeSsR3QICwRgqKa32/6c2qC69HVcGyd/a5feAqqitUfvB5jknU9I4mGLCb1i3D/lF6Q+SJCiHqWYKTgBFSVtP04S69I4sXg7Z51lyQYcSfnp/Q2J6IvjPul2l6zWKXj2oMlWu8+EDy87HNOW4QlYsJIoKGSK3p2gkAEpGdECFEvIKJ+UdCs/W0/zs2HaECCEfdinbw6rvV9L/2dWjsm7yhs/699rq9HsbMGUgtrOGNWqbxjQwp1aYPd5dfNGZGeESEE2D5UU1kMp+sKYfad5pg2OYEEI+7CVFO/OF7C+Nb39wuFKY+r7e+fb1/e+vn0zKRBrdJ7SosGwL/4lC5tsDtrz0gvfdshhHANtmbUpP6g5giG93brHlYJRtxF1j61LLRvCES0cb7GiNtVPZCqIvj+Tx27vqY1yKTRp2fkh6O5pNYFIw1X73VbVSVQmq22pWdECAG2Z9R0giEakGDEfVgXxxvX9mJjRg+Y8Re1vXN5fSZMe5RkqfolBiNEDW7/edqpssbElhP51p4R68RPd2ZJ6/XvpnqyhBDCMkyTewhqq1reV9Pg2Fq17cZDNCDBiPuw1BeJb8N8kYYSJ8Lg60Ezw+rF6o+3PSxdhhH9Gld+dZKtJ/OpqjVT5JegbrC8kbszyaQRQpwvJA78wsBcCzkHW943ez+UZIKXv1q93Y1JMOIONK2+Z6SpYmetmfY0ePqqcsGHvmxfG/QsdgbWVXpje9f1yhScbH9g5Sokk0YIcT6Doe1DNZYhmqRJav0uNybBiDsoPK2iX6MXxI60/fjQBJj4oNr+9g9QU2H7OSzBiE6ZND8cVcHI0MHDwOCh5s+UZOrSFruRTBohRFPamlHTSYZoQIIR92CpLxKT3P4hkokPQnAPVaVvy79sP17HTJrT+WWk5pXhaTQwoV80hPVUd7j7vBHJpBFCNMUSjLSUUVNxDtK3qW0JRoRTtLQ4Xlt5B6jhGoCfXoLis20/trxABTGgsnOczDJEM6pnGEG+XvU9Ce6eUSNzRoQQTbEM02TtB7Op6X1OfA+aCSIHqt5vNyfBiDtIa5BJ0xFDblTVW2vKYd1TbT/OkoUT2lNNrHKyDUdyAJjcv26VXsscC3fuGaksgvK62i8yZ0QI0VC33uAVALUVkHes6X060RANSDDi+ioK62dUd6RnBNTEqJl/Bgyw98P6Lr7W6DhEU1ljYsvJfKDBKr3WnhE3zqixBFIB3cEnSN+2CCFci9EDooeo7aaGasxmOG4JRty7voiFBCOuzrI4XngvCOze8fPFjoARt6ntbx5Rf9St0bHY2bbUAiprzEQH+zIguu5N2zLHwp17RiyBlPSKCCGaYs2oaWISa+ZuVffJO6jjH1JdhAQjrs5aX8SOf3CXP6H+iM+mwN4PWt9fxzVpLPNFJvWLrF+l19Izci61bcGUK5L5IkKIlrSUUWMZoul9mT6LljqABCOuzlpfpIPzRRoKioJJv1Pb655qeanq6jLIrxuz1GGYZsNRy3yRyPobQ+JVmnNtJRSfcXqb7EIyaYQQLWm4Rs35NZU6SQn4hiQYcWUNF8ezZ88IwLhfqqGf0mz46W/N75d9QFVvDegOQdH2bUMr0gvKOZlbhofRwMS+EfV3eHhCWKLadteMGukZEUK0JHKg+tBVWVSfzQhq0dMzO9V2n6n6tM0BJBhxZVl71Wxq31BVht2ePH1g+nNqe8urzU8G1bHYmSWLZlRCGMG+53VFuntGjVRfFUK0xNMbug9U2w2Hao6vBzRVZiE4RpemOYIEI64srUF9kbYujmeLfjOg9+VgqoZvH296Hx0zaazzRRoO0Vi4c0ZNeYEqWASqd0oIIZrScKjGohMO0YAEI66tvYvjtZXBANOfV+XVD3+liuicT6c1aSprTGw+cV5Kb0PunFFjCaCCYlQxOiGEaIolg9GSRGA2wfF1aluCEeEUHV0cr626D4Cx96jt1YvBVFt/n6kGcg6pbScP0+w4dY6KGhPdg3wYFBN84Q7uXIVV5osIIdri/IyajB1QWaiG7nuM1qtVDtGuYOTVV18lMTERX19fxo0bx7ZtbSue9cEHH2AwGLjuuuvac9mu5dwpNbnU6KVqgzjS5EfBLxxyD8HOZfW35x5WQzg+IfUTRp3EMl+kUUpvQ5a5FudONV8u2VVJJo0Qoi2iBgMGKM2C0pz6IZo+U9RE/k7E5mDkww8/ZNGiRTz55JOkpKSQnJzM9OnTycnJafG4U6dO8fDDD3PJJZe0u7FdiqVXJHY4ePk59lp+YXD5Y2r7u2fVnAZoMEQzVA3pONGGulV6rSXgzxccBx4+KlgqSndiy+xAekaEEG3hEwjd+qjtzL2ddr4ItCMYeemll7jnnntYsGABgwYN4vXXX8ff35+lS5c2e4zJZOK2227jj3/8I716yafBNknbor47q7reyDuh+2DVBbjheXWbTsXOMs6VczynFA+jgYsbpvQ2ZDRCeJLadrd5I5JJI4RoK8v/32Nr6iayGqD3FF2b5Ag2BSPV1dXs3LmTqVPrc5uNRiNTp05ly5YtzR739NNP0717dxYuXNim61RVVVFcXNzoq8uxLo7npGDEwxNm1AUh25dA9kHdMmksWTQj4kMJ8WuhuqA7ZtRoGuTXtVd6RoQQrbH8/015W33vMRICm5jU7+ZsCkby8vIwmUxERUU1uj0qKoqsrKwmj9m4cSNLlizhzTffbPN1nn/+eUJCQqxf8fHxtjTT/VWcU/M3wHGZNE3pNQkGXqOWpV79aP1qvU7OpLEEI01m0TTkjhk15flQVaS2LT07QgjRHMv/39pK9b0TDtGAg7NpSkpKuP3223nzzTeJiGimu70JixcvpqioyPqVnu5mcwI6Kn27+h7e2/kR8LRn1FyM1B+guhQ8fe1fcK0FVbUmNp/IA1qYL2Lhjhk1lsApOM7xc4GEEO7v/GHyvtP0aYeD2TQdNyIiAg8PD7Kzsxvdnp2dTXT0haXCT5w4walTp7jmmmust5nrFjbz9PTkyJEj9O59YVe1j48PPj4+tjStfarL4ehqGHSdY4qKtVd6g2JnzhaeBBMeqC8RHzXYqbO2d5w6R3m1iYjAZlJ6G3LHKqySSSOEsIV/uFqPqygd/CMgxsHZlTqx6R3Y29ubUaNGsX79euttZrOZ9evXM378+Av2HzBgAPv27WP37t3Wr2uvvZbLLruM3bt36zv8YjbBv8bAJwsgbbN+7WhKmo7BCMDFiyCwLrh08hDND0frV+k1GlvJ4LH0jBSeblwfxZVJJo0QwlaW/8N9p7nWB2c7svkj76JFi7jjjjsYPXo0Y8eO5eWXX6asrIwFCxYAMH/+fHr06MHzzz+Pr68vQ4YMaXR8aGgowAW3O53RQ+Vqp7wFKe9A4sX6tseitrp+ESRnTV49n08gXPdv+O4ZGHWHUy9tqS/S6nwRUBVMPf3U+j2Fp90jO0UyaYQQtrrol1CWAxN+rXdLHMbmYGTOnDnk5ubyxBNPkJWVxfDhw1m9erV1UmtaWhpGd4ncRs5XwcjBL+DKv4JviN4tqlscr1IVIYvoq187+kxRX050trCCo9mlGA1wSXMpvQ0ZjWptl5wDKqPGHd7gpWdECGGrpEvh7nV6t8Kh2jUZ4IEHHuCBBx5o8r4NGza0eOzy5cvbc0nH6DFKLdOcewj2fwqj79K7RY3Xo3FyoTG9WbJohseHEurv3baDutUFI/knXH9il6bVpyG7Q+AkhBBO4iZdGA5iMMCIeWo75R1922JhnbzqxJReF1E/RNNKFk1D7pRRU5qjMpQMRqeX1xdCCFfWtYMRgORb1PovZ1Mg+4C+bdG0Bj0jOs0X0Ul1rZlNxy0pvTakM7tTRo0lYAqJA08nZIsJIYSbkGAkIAL6z1Tbu1bo25aCk1CWCx7ejl8cz8XsOF1AWbWJiEBvhsTaMHfHnXpGZL6IEEI0SYIRgBG3q+97PoDaKv3aYV0cbwR4+erXDh38UDdf5NK+bUjpbcjSM1KYpjKRXJlk0gghRJMkGAGVNRIUCxUFcORr/drRcPJqF2OtL2LLEA1AYBR4B4JmVum9rkx6RoQQokkSjICqOTL8VrWt51CNpWdEr2JnOsksquBwVglGg+oZsYnB4D6r90omjRBCNEmCEYsRt6nvx9dDUYbzr19eALmH1XYX6xmxDNEkx4cSFtDGlN6G3GHeSMO0XukZEUKIRiQYsQjvBYmXABrsfs/510/fpr5366sm1XYhlvoik/q1c1FAd8ioKcmEmnIweEBYT71bI4QQLkWCkYYsE1l3rYC6Bf2cpovWF6kxNUzptaG+SEPu0DNiCZRCE8DDS9+2CCGEi5FgpKGB14BPsJoIeeon5147rW6+SBerL7Lz9DlKqmoJD/BmWI92luO39oyctF/D7E0yaYQQolkSjDTk7Q9Db1Lbu5xYkbW2ShVdgy43eXWDNaU3wraU3oYsPSNF6VBTaaeW2Zlk0gghRLMkGDmfZajm4JdQcc4518zcoxbH8+8G3fo455ouol0l4M8XEKF6tNDg3Cm7tMvuJJNGCCGaJcHI+WJHQPfBYKqCfZ8455oNS8B3ocXxsosrOZxVgsEAl7Z38irUpff2UtuuOm9EekaEEKJZEoycz2CAkZaJrE4aqrHWF+lak1ctKb3D4kIJb09Kb0OunFFjNsO5VLXdrZe+bRFCCBckwUhThs1R68Nk7oHMvY69VhdeHG/DUTVE0+6U3oZcOaOm+IwahjN6QkiC3q0RQgiXI8FIU/zDof+VatvRFVkLTkJ5Hnj4QOxwx17LhdSazPx0rB2r9DbHlXtGLAFSWCJ4eOraFCGEcEUSjDTHMlSz90PHZmikbVHfY0d0qWXlU9IKKamsJczfi+S40I6f0Noz4oLpvTJfRAghWtSlg5Gi8ho+3J6GyaxdeGevyyA4DioL4cgqxzXCMkTT5VJ61RDNJX0j8WhvSm9Dlp6R4jNQXd7x89mTZNIIIUSLumwwYjJrTPv7Dzzy6T42n8i7cIeGi+elOHAiaxdcHO9odgkf71Tr/9hliAbU0JpvqNq2TBZ1FdaeEZm8KoQQTemywYiH0cCMIdEAfLyjmYXxLIvnndwAhWn2b0R5AeQdVdtdZHG8zSfyuPG1zeSWVNE7MoArBkfb7+SuOm9Eqq8KIUSLumwwAnDzqHgAVh/Ioqi85sIdwhIhaRIOWzzP0isS0U99su/kPt91hjuWbqOkspYxiWF88ssJBPrYcUKnK2bUmE31hdhkzogQQjSpSwcjQ3oEMyA6iOpaM1/uPdv0TtbF8961/+J5lsmrnbxXRNM0Xv3+OL/5cDc1Jo2rhsbwzsJxhHW0tsj5XLFnpCgdTNUqVTwkTu/WCCGES+rSwYjBYOCmUeoN4pMd6U3vNPBq8A2BojRI3WDfBlgWx0sYb9/zupBak5nHPt/PC2uOAHDPJUm8MncEvl4e9r+YK2bUWAKjsCQ1D0kIIcQFunQwAnD9iB54Gg3sySjiaHbJhTt4+cHQ2WrbnjVHaqvg7C613Y7JqydzS7noufXMX7qN9AIXyx6pU1ZVy73v7OS9n9MwGOCpawbx2FWD2r8gXmss1U1dqWdEMmmEEKJVXT4Y6Rbow+UD1CJtHzfXOzJinvp+6Cs16dQezu5W69/4R7Qry+LNn06SVVzJj0dzmf7yj7y1+RTmplKUdZJbUsUt/9nKd4dz8PE08tpto7hzYpJjL2rpGSnNgqpSx16rrSSTRgghWtXlgxGA2aPVRNbPdp2hxtTEvJDY4RA9tG7xvI/tc9H0BvVFbFwcr6iihs93qTkuA6KDKK828eSXB7jlP1tJzSuzT/s64HhOKdf/exP7zhQRHuDN+/deZM1ccii/ULXyMbjOUI1k0gghRKskGEHVuogI9CGvtJoNdYu3XWDEfPXdXovnWdejsX3y6sqUDCpqTPSPCuLr/3cJz8wajL+3B9tOFTDj5R9588eTTRdyc4Ltpwq48bXNZJyroGc3f1beN4GRCWHOa4CrZdRI9VUhhGiVBCOAp4eRG0b2AOCj5oZqht6k1o/J2qeGWDpC0xoUO7Nt8qqmabyz9TQA88b3xGg0cPv4RNb85lIu7hNBVa2ZP319iJte38zxnCbmwDjQqr2Z3PbfnymqqGF4fCgr75tAYkSAU9vgUhk1plooVM+V9IwIIUTzJBipc3NdVs33h3PIK626cAf/cJVZAx3vHck/DuX54OkLMck2HbrlRD4nc8sI8Pbg+hE9rLfHh/vzzsKx/PmGoQT5eLIrrZAr/7GRV78/Tm1TQ092pGka//3pJPe/l0J1rZlpg6J4/56L6Baow1o7rpRRU3gazLXqeQ6K1bs1QgjhsiQYqdM3Kojk+FBqzRqf7zrT9E6Wiaz7PoaaivZfzDJEEzsSPG2rtWHpFblhZNwFBcMMBgO3jE1gzW8vZXL/SKpNZl5Yc4Tr/72ZQ5nF7W9vC0xmjT/+7yDPrjoEwB3je/L6vFH4eeuUxupKGTWWgCi8FxjlpSaEEM2R/5ANWHpHPt6RgaY1MeciaTKEJEBlkcqsaS/r5FXb5otkFVXy7cFsAOZd1LPZ/WJD/Vh25xj+dnMywb6e7DtTxLX/2sjL645SXWu/XpLKGhO/encnyzefAuCxKwfy1LWD7bPwXXu50pwRyaQRQog2kWCkgWuSY/HxNHIku4R9Z4ou3MForF+vZtfb7b+QdfKqbfVF3t+mVhgemxRO/+igFvc1GAzcOCqOdYsmMW1QFDUmjZfXHePaf21kf1OPzUb5pVXMfXMraw5k4+1h5F+3juCeS3thsDEzyO4sczPKcqHSMb1BbSaZNEII0SYSjDQQ4ufV+uJ5w28FDJD6Y/2aI7Yoy1NzRgDix7b5sBqTmfe3qcX6bm+hV+R83YN9+c/to/jn3BGE+XtxOKuEWa9u4oU1h6mqNdnUdItTeWXc+NpmdqUVEuLnxYq7x3H1MBeZE+ETBAGqbozuvSOSSSOEEG0iwch5LIvnfbH7DJU1TbxZhyZAr8lqe9e7tl/AkkUTOcCmxfHWHswmp6SKiEAfptu40q3BYODa5FjWLprEVcNiMJk1Xv3+BFf9cyO70s7ZdK5daee44bXNnMovJy7Mj0/vG8/YJBdb5M9VMmqkZ0QIIdpEgpHzTOjdjR6hfhRX1lrnZ1xgZN3iebvfVauy2qKd9UXe2aImrs4dG4+3Z/uetohAH169dSSvzxtJRKA3x3NKufG1zTz39aGmA6/zfHsgi7lvbqWgrJqhPUJY+asJ9One8nCRLlwho6a2GgrTGrdHCCFEkyQYOY/RaODGupojzZaH738V+IZC8Rk4+b1tF7DWF2n7fJHjOSVsOZmP0QBzxybYdr0mzBgSw9rfTuL6ET0wa/CfH08y8x8/sf1U86Xu39p8il+s2ElljZnL+kfywb0X0T3It8NtcQhXyKgpPA2aGbwCIMgJ1WeFEMKNSTDShJvqhmo2Hs/jbGETKbxevjBsjtpOsaHmSE1l/eJ4NvSMrNiqPmFPHRhFbKhf26/XgrAAb/4+ZzhL7hhNVLAPqXllzH5jC099eYDy6lrrfmazxnNfH+LJLw+gaSoYenP+aALOSyt2Ka6QUdMwk0bvSb1CCOHiJBhpQkI3f8YlhaNpqvR6kyxDNYdXQVl+2058dheYqtUEyzame5ZV1fLpTtWG28e3feJqW00ZGMW3v53E7NFxaBos33yK6S//yOYTeVTWmPh/H+ziPz+q4Y7fTe/Pc9cPwdPDxf9sLHM08o7ZPoxmL9b5IpLWK4QQrXHxdxX9WBbP+3hnMzVHooeq6qnmGtj3UdtO2rC+SBs/LX+x+ywlVbUkRQQwsXdE265joxA/L/56UzJv3zWWHqF+pBdUcOubP3PF33/kq72ZeHkY+PucZO6/rI/+qbttEdFPDaNVFsLJDfq0QTJphBCizSQYacbModEE+nhyOr+cbanNzKUYUdc7kvKOWm+mNWl180XaWF9E0zTe3nIKgNvGJWB0cDGxS/tFsvo3lzDvIjUvJa2gnCAfT95aMJbrR8Q59Np25ekDw2arbXstbGgryaQRQog2k2CkGf7enlw1NAZQvSNNGnqzWnck5wCcTWn5hGazzZNXU9LOcTirBF8vozXl2NGCfL149rqhvH/PRdw6LoFPfzWBCX0c0yPjUCMaDKOVNz8x12HyLaXgJRgRQojWSDDSgptHq96Ar/dlUlZVe+EOfqEw8Bq1vWtFyyfLPwYVBSp4iR7Wputb0nmvTY4lxN+rrc22i/G9u/Hc9UPpF+WCqbttETNM/Z5N1bC3jcNo9lJTCUV1mVjSMyKEEK2SYKQFo3qG0SsigPJqE6v2ZTa9k+UT+L5PoLq8+ZNZ6ov0GNWmxfHySqv4el8WALdflGhDq4XVyPnq+642DqPZy7lTgAbeQRAQ6bzrCiGEm5JgpAWW9V0APmmuPHziJRDaE6qK4dCXzZ/MxiGaj3akU20ykxwfytC4EFuaLSyG3gQePpC9vz6l2hkaZtK4w4RfIYTQmQQjrbhxZBxGA2w7VUBqXtmFOxiNMGKe2m5pqMaGxfFMZo13t9q+Do04j19Y24fR7KlA5osIIYQtJBhpRXSIL5f2U13tn+xspiKrZfG8Uz81XYK8NLf+03L8mFavueFIDmcKKwj19+LqYTHtbLkA6uvB7PsEapooYOcI+ZJJI4QQtpBgpA0smSyf7jyDydzE3IOQOOgzRW039QncUl8kcqD6tN6Kd7aqiauzR8fj6+XRrjaLOomXqsUNq4rgYAvDaPZUIDVGhBDCFhKMtMHUQd0J9fciq7iSjcfzmt7JMlSz+70Lq36mNSh21orT+WX8cDQXULVFRAcZjTDcMozmpJojlrRe6RkRQog2kWCkDXw8PZiVHAu0tHjeleAXDiWZcHx94/usk1fHt3qt935OQ9NgUr9IenYL6EizhUVrw2j2VFMBxXWTnaVnRAgh2kSCkTa6ua48/LcHsykqr7lwB08fSL5Fbe96u/72mgo4u1ttt7I4XmWNiQ/rgh2ZuGpHofHQ+3K1vetdx16rIFV99w0B/3DHXksIIToJCUbaaHBsMANjgqmuNfPlnjNN72QZqjnyDZTVDeec3aXWrwmMgrDEFq+xam8mheU19Aj147IB3e3XeNHyMJo9NZwvImm9QgjRJhKMtJHBYODmupojHzVXcyRqMMSOBHMt7PlA3Za2RX2Pb31xPMvE1VvHJeDh4HVoupwBV9UNo52FE9857jqSSSOEEDaTYMQG143ogZeHgX1nijicVdz0TpZUUkvVz7S2FTvbl1HE7vRCvDwMzBnjnHVouhRPHxg2R22nvN3yvh0hmTRCCGEzCUZsEB7gzZQBUQB83FzvyJAbwdMPcg9DxvY2V15dUdcrcuXQGCICfezWZtFAU8No9iaZNEIIYTMJRmxkWTzv811nqDGZL9zBNwQGzVLba5+AykLw8m9xcbyi8hq+qJuHIhNXHSh6CMSOUHN49n7omGtIz4gQQthMghEbTeoXSWSQD/ll1Xx3OKfpnSxDNZb5Ij1GgUfzq+5+kpJBZY2ZAdFBjOrZelE00QGWhQ1THLB4XnWZSu0GtS6NEEKINmlXMPLqq6+SmJiIr68v48aNY9u2bc3uu3LlSkaPHk1oaCgBAQEMHz6cd95xUvEpB/D0MHLDiB5ACzVHek6EsKT6n1tI6TWbNesQze3je2KQDAzHGnIjePpC7iE4k2Lfc1tqmPiFt6nSrhBCCMXmYOTDDz9k0aJFPPnkk6SkpJCcnMz06dPJyWm6lyA8PJzHHnuMLVu2sHfvXhYsWMCCBQtYs2ZNhxuvF8tQzfdHcskpqbxwB4Ohfn4CtDhfZPOJfFLzygj08eS64T3s3VRxPr/Q+mG0XXaeyCqZNEII0S42ByMvvfQS99xzDwsWLGDQoEG8/vrr+Pv7s3Tp0ib3nzx5Mtdffz0DBw6kd+/ePPjggwwbNoyNGzd2uPF66dM9iBEJoZjMGp/vaqbmyPBbweilJrPGNb843jtbTwFw48geBPh4OqC14gKWoZp9n6qhFXuR+SJCCNEuNgUj1dXV7Ny5k6lTp9afwGhk6tSpbNmypdXjNU1j/fr1HDlyhEsvvbTZ/aqqqiguLm705Wosi+d9vCMDram5B8GxcOcquONL9Wm8CZlFFaw9mA3APJm46jw9J6oCdNUl9l08z5JJEy7zRYQQwhY2BSN5eXmYTCaioqIa3R4VFUVWVlazxxUVFREYGIi3tzdXXXUVr7zyCtOmTWt2/+eff56QkBDrV3y869XduDo5Bl8vI8dyStmTUdT0TgnjIH5ss+d4/+c0zBpc1CucvlFBDmqpuIDRWD+MZs/F8wpkmEYIIdrDKdk0QUFB7N69m+3bt/OnP/2JRYsWsWHDhmb3X7x4MUVFRdav9PRmJorqKNjXixmDo4EWJrK2oLrWzPvbLevQJNqzaaItkm8FgxFOb6qf69FRlvNIz4gQQtjEpmAkIiICDw8PsrOzG92enZ1NdHR08xcxGunTpw/Dhw/noYce4qabbuL5559vdn8fHx+Cg4Mbfbkiy+J5X+45S2WNbeudfHswi9ySKiKDfLhicFTrBwj7CukBvaeo7V0rOn6+ymIoq5vELT0jQghhE5uCEW9vb0aNGsX69eutt5nNZtavX8/48ePbfB6z2UxVVZUtl3ZJ43t1o0eoHyWVtaw50PwwVVPe2aLSeeeOTcDLQ8q96MJSD2b3e2Cq7di5LGm9/hGq8J0QQog2s/ldcNGiRbz55pu89dZbHDp0iPvuu4+ysjIWLFgAwPz581m8eLF1/+eff561a9dy8uRJDh06xN/+9jfeeecd5s2b19wl3IbRaOCmusXzmi0P34Sj2SX8nFqAh9HA3LGuNx+my+g3E/y7QWkWHF/XsXPJfBEhhGg3m3NJ58yZQ25uLk888QRZWVkMHz6c1atXWye1pqWlYTTWxzhlZWX86le/IiMjAz8/PwYMGMCKFSuYM2eO/R6Fjm4aFcc/1h9j04k8zhRW0CPUr9VjLEXOpg2MIiak9f2Fg3h6w7BbYOuraiJr/xntP5c1k0aCESGEsJVBazIv1bUUFxcTEhJCUVGRS84fmfufrWw5mc+iaf34f1P6trhvaVUtFz23ntKqWt69exwT+0Q4qZWiSTmH4N8XgdETFh2CwO7tO89nv4Q978Plf4BLf2ffNgohhJtq6/u3TFawA0tF1k92ZmA2txzbfb7rDKVVtfSKDGBC727OaJ5oSfeB0GM0mGthzwftP0++FDwTQoj2kmDEDmYOiSHQx5O0gnJ+Ti1odj9Nq1+HZt44WYfGZVhrjqxo/+J5MmdECCHaTYIRO/Dz9uCa5BgAPt7ZfM2RHafPcTirBF8vIzfWTXwVLmDIjapsf94RyNhu+/EVhVCer7alxogQQthMghE7uamuPPw3+7IorWo6TdSSznvd8B6E+Hk5rW2iFb7BMPg6tZ3SjsXzLL0igVHgI5V0hRDCVhKM2MnIhFB6RQZQUWNi1d6zF9yfW1LFN/szAVmHxiVZFs878BlUldp2rGTSCCFEh0gwYicGg6HR4nnn+2hHOjUmjREJoQzpIUWxXE7PCWqIpboUDn5u27HW+SIyRCOEEO0hwYgd3TCyB0aDmhtyMrf+07XJrPFu3cTV26VXxDUZDI0nstpCMmmEEKJDJBixo6hgXyb1iwTg4531vSPfHc7hbFElYf5eXDk0Rq/midZYFs9L2wJ5x9p+nGTSCCFEh0gwYmez6xbPW5mSgamu5sg7db0is8fE4+vloVvbRCuCY6DvFWp71zttP056RoQQokMkGLGzKQOjCPP3Iru4ih+P5XIqr4wfj+ZiMMBtY2WIxuVZhmp2vw+mmtb3Ly+AykK1LWm9QgjRLhKM2Jm3p5FZw3sA8MmODN79WfWKTO4XSUI3fz2bJtqi3wwIiISyHDi2tvX9Lb0iQbHgLc+vEEK0hwQjDmApD7/2YDYfbldF0G4fL70ibsHDC5JvUdttGaqR+SJCCNFhEow4wODYEAbFBFNtMlNcWUtcmB+T+rVzATbhfJaaI0fXQEl2y/ta54vIEI0QQrSXBCMOYukdAbhtXE88jLIOjduI7A9xY0EzqZV4WyI9I0II0WESjDjIdcN7EODtQYC3B7NHyzo0bmdkXe/IrndaXjxPMmmEEKLDJBhxkLAAb7789cV8+euL6Rboo3dzhK0GXw9eAZB/HNJ/bnofTYOCulLw0jMihBDtJsGIA/WODKR3ZKDezRDt4ROkAhKAlGYmspblQVUxYICwJKc1TQghOhsJRoRozsiGi+eVXHi/Zb5ISBx4+TqvXUII0clIMCJEc+LHQbe+UFOmApLzSSaNEELYhQQjQjSn4eJ5TQ3VSCaNEELYhQQjQrQkeS4YPCBjG+QeaXyfZNIIIYRdSDAiREuCoqDfdLV9fkVW6RkRQgi7kGBEiNZYKrLu+aB+8TxNg/y6tF7pGRFCiA6RYESI1vS9AgKjoCwXjq5Wt5Vmq4mtBiOEJeraPCGEcHcSjAjRGg/PBovnrVDfLfNFQuLB01ufdgkhRCchwYgQbWEZqjn2LRRnynwRIYSwIwlGhGiLiL6QMB40M+x5TzJphBDCjiQYEaKtLDVHdq1Qa9aA9IwIIYQdSDAiRFsNug68A9XieMfXq9ukZ0QIITpMghEh2sonEIbcoLZrK9R36RkRQogOk2BECFtYJrKCqswamqBfW4QQopOQYEQIW8SNgYj+ajusJ3h46dseIYToBCQYEcIWBgOMnK+2owbr2xYhhOgkPPVugBBuZ9wvwScIel+md0uEEKJTkGBECFt5eMKoO/RuhRBCdBoyTCOEEEIIXUkwIoQQQghdSTAihBBCCF1JMCKEEEIIXUkwIoQQQghdSTAihBBCCF1JMCKEEEIIXUkwIoQQQghdSTAihBBCCF1JMCKEEEIIXUkwIoQQQghdSTAihBBCCF1JMCKEEEIIXbnFqr2apgFQXFysc0uEEEII0VaW923L+3hz3CIYKSkpASA+Pl7nlgghhBDCViUlJYSEhDR7v0FrLVxxAWazmbNnzxIUFITBYLDbeYuLi4mPjyc9PZ3g4GC7nddVdaXHK4+18+pKj1cea+fVVR6vpmmUlJQQGxuL0dj8zBC36BkxGo3ExcU57PzBwcGd+o/hfF3p8cpj7by60uOVx9p5dYXH21KPiIVMYBVCCCGEriQYEUIIIYSuunQw4uPjw5NPPomPj4/eTXGKrvR45bF2Xl3p8cpj7by62uNtjVtMYBVCCCFE59Wle0aEEEIIoT8JRoQQQgihKwlGhBBCCKErCUaEEEIIoatOH4y8+uqrJCYm4uvry7hx49i2bVuL+3/88ccMGDAAX19fhg4dytdff+2klnbM888/z5gxYwgKCqJ79+5cd911HDlypMVjli9fjsFgaPTl6+vrpBa331NPPXVBuwcMGNDiMe76vCYmJl7wWA0GA/fff3+T+7vbc/rjjz9yzTXXEBsbi8Fg4PPPP290v6ZpPPHEE8TExODn58fUqVM5duxYq+e19XXvDC091pqaGh555BGGDh1KQEAAsbGxzJ8/n7Nnz7Z4zva8Fpyhtef1zjvvvKDdM2bMaPW8rvi8QuuPt6nXsMFg4IUXXmj2nK763DpKpw5GPvzwQxYtWsSTTz5JSkoKycnJTJ8+nZycnCb337x5M3PnzmXhwoXs2rWL6667juuuu479+/c7ueW2++GHH7j//vvZunUra9eupaamhiuuuIKysrIWjwsODiYzM9P6dfr0aSe1uGMGDx7cqN0bN25sdl93fl63b9/e6HGuXbsWgJtvvrnZY9zpOS0rKyM5OZlXX321yfv/+te/8s9//pPXX3+dn3/+mYCAAKZPn05lZWWz57T1de8sLT3W8vJyUlJSePzxx0lJSWHlypUcOXKEa6+9ttXz2vJacJbWnleAGTNmNGr3+++/3+I5XfV5hdYfb8PHmZmZydKlSzEYDNx4440tntcVn1uH0TqxsWPHavfff7/1Z5PJpMXGxmrPP/98k/vPnj1bu+qqqxrdNm7cOO0Xv/iFQ9vpCDk5ORqg/fDDD83us2zZMi0kJMR5jbKTJ598UktOTm7z/p3peX3wwQe13r17a2azucn73fU51TRNA7TPPvvM+rPZbNaio6O1F154wXpbYWGh5uPjo73//vvNnsfW170ezn+sTdm2bZsGaKdPn252H1tfC3po6rHecccd2qxZs2w6jzs8r5rWtud21qxZ2uWXX97iPu7w3NpTp+0Zqa6uZufOnUydOtV6m9FoZOrUqWzZsqXJY7Zs2dJof4Dp06c3u78rKyoqAiA8PLzF/UpLS+nZsyfx8fHMmjWLAwcOOKN5HXbs2DFiY2Pp1asXt912G2lpac3u21me1+rqalasWMFdd93V4oKR7vqcni81NZWsrKxGz11ISAjjxo1r9rlrz+veVRUVFWEwGAgNDW1xP1teC65kw4YNdO/enf79+3PfffeRn5/f7L6d6XnNzs5m1apVLFy4sNV93fW5bY9OG4zk5eVhMpmIiopqdHtUVBRZWVlNHpOVlWXT/q7KbDbzm9/8hokTJzJkyJBm9+vfvz9Lly7liy++YMWKFZjNZiZMmEBGRoYTW2u7cePGsXz5clavXs1rr71Gamoql1xyCSUlJU3u31me188//5zCwkLuvPPOZvdx1+e0KZbnx5bnrj2ve1dUWVnJI488wty5c1tcRM3W14KrmDFjBm+//Tbr16/nL3/5Cz/88AMzZ87EZDI1uX9neV4B3nrrLYKCgrjhhhta3M9dn9v2cotVe4Vt7r//fvbv39/q+OL48eMZP3689ecJEyYwcOBA3njjDZ555hlHN7PdZs6cad0eNmwY48aNo2fPnnz00Udt+rThrpYsWcLMmTOJjY1tdh93fU5FvZqaGmbPno2mabz22mst7uuur4VbbrnFuj106FCGDRtG79692bBhA1OmTNGxZY63dOlSbrvttlYnlrvrc9tenbZnJCIiAg8PD7Kzsxvdnp2dTXR0dJPHREdH27S/K3rggQf46quv+P7774mLi7PpWC8vL0aMGMHx48cd1DrHCA0NpV+/fs22uzM8r6dPn2bdunXcfffdNh3nrs8pYH1+bHnu2vO6dyWWQOT06dOsXbvW5qXlW3stuKpevXoRERHRbLvd/Xm1+Omnnzhy5IjNr2Nw3+e2rTptMOLt7c2oUaNYv3699Taz2cz69esbfXJsaPz48Y32B1i7dm2z+7sSTdN44IEH+Oyzz/juu+9ISkqy+Rwmk4l9+/YRExPjgBY6TmlpKSdOnGi23e78vFosW7aM7t27c9VVV9l0nLs+pwBJSUlER0c3eu6Ki4v5+eefm33u2vO6dxWWQOTYsWOsW7eObt262XyO1l4LriojI4P8/Pxm2+3Oz2tDS5YsYdSoUSQnJ9t8rLs+t22m9wxaR/rggw80Hx8fbfny5drBgwe1e++9VwsNDdWysrI0TdO022+/XXv00Uet+2/atEnz9PTUXnzxRe3QoUPak08+qXl5eWn79u3T6yG02X333aeFhIRoGzZs0DIzM61f5eXl1n3Of7x//OMftTVr1mgnTpzQdu7cqd1yyy2ar6+vduDAAT0eQps99NBD2oYNG7TU1FRt06ZN2tSpU7WIiAgtJydH07TO9bxqmsoaSEhI0B555JEL7nP357SkpETbtWuXtmvXLg3QXnrpJW3Xrl3WDJI///nPWmhoqPbFF19oe/fu1WbNmqUlJSVpFRUV1nNcfvnl2iuvvGL9ubXXvV5aeqzV1dXatddeq8XFxWm7d+9u9BquqqqynuP8x9raa0EvLT3WkpIS7eGHH9a2bNmipaamauvWrdNGjhyp9e3bV6usrLSew12eV01r/e9Y0zStqKhI8/f311577bUmz+Euz62jdOpgRNM07ZVXXtESEhI0b29vbezYsdrWrVut902aNEm74447Gu3/0Ucfaf369dO8vb21wYMHa6tWrXJyi9sHaPJr2bJl1n3Of7y/+c1vrL+bqKgo7corr9RSUlKc33gbzZkzR4uJidG8vb21Hj16aHPmzNGOHz9uvb8zPa+apmlr1qzRAO3IkSMX3Ofuz+n333/f5N+t5TGZzWbt8ccf16KiojQfHx9typQpF/weevbsqT355JONbmvpda+Xlh5rampqs6/h77//3nqO8x9ra68FvbT0WMvLy7UrrrhCi4yM1Ly8vLSePXtq99xzzwVBhbs8r5rW+t+xpmnaG2+8ofn5+WmFhYVNnsNdnltHMWiapjm060UIIYQQogWdds6IEEIIIdyDBCNCCCGE0JUEI0IIIYTQlQQjQgghhNCVBCNCCCGE0JUEI0IIIYTQlQQjQgghhNCVBCNCCCGE0JUEI0IIIYTQlQQjQgghhNCVBCNCCCGE0JUEI0IIIYTQ1f8HXZEnKVtfdVwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "only prediction time use"
      ],
      "metadata": {
        "id": "-YX8xg8VkMoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 11. Optional: Real-time tracking + behavior detection\n",
        "# -----------------------------\n",
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    tracks_dict = {}\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detect humans\n",
        "        results = yolo_model.predict(frame, classes=[0])\n",
        "        detections = []\n",
        "        for r in results:\n",
        "            for box in r.boxes.xyxy.cpu().numpy():\n",
        "                x1, y1, x2, y2 = box[:4]\n",
        "                detections.append(([x1,y1,x2-x1,y2-y1], 1.0, 'person'))\n",
        "\n",
        "        # Track humans\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            track_id = track.track_id\n",
        "            l,t,r,b = map(int, track.to_ltrb())\n",
        "            crop = frame[t:b, l:r]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop = cv2.resize(crop, img_size)\n",
        "            crop = crop/255.0\n",
        "\n",
        "            if track_id not in tracks_dict:\n",
        "                tracks_dict[track_id] = []\n",
        "            tracks_dict[track_id].append(crop)\n",
        "\n",
        "            if len(tracks_dict[track_id]) >= num_frames:\n",
        "                clip = np.array(tracks_dict[track_id][:num_frames])\n",
        "                clip = np.expand_dims(clip, axis=0)\n",
        "                pred = model.predict(clip)\n",
        "                pred_class = class_names[np.argmax(pred)]\n",
        "                print(f\"Person {track_id}: {pred_class}\")\n",
        "                tracks_dict[track_id] = tracks_dict[track_id][8:]\n",
        "\n",
        "    cap.release()"
      ],
      "metadata": {
        "id": "1dJPkt4Wxc3f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update code final class find"
      ],
      "metadata": {
        "id": "tEcrnUNNxdwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    tracks_dict = {}      # store frame sequences per person\n",
        "    tracks_preds = {}     # store all predictions per person\n",
        "    frame_count=0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_count += 1\n",
        "        # Skip 9 frames, process only 1 every 10\n",
        "        if frame_count % 10 != 0:\n",
        "            continue\n",
        "        # Detect humans\n",
        "        results = yolo_model.predict(frame, classes=[0])\n",
        "        detections = []\n",
        "        for r in results:\n",
        "            for box in r.boxes.xyxy.cpu().numpy():\n",
        "                x1, y1, x2, y2 = box[:4]\n",
        "                detections.append(([x1,y1,x2-x1,y2-y1], 1.0, 'person'))\n",
        "\n",
        "        # Track humans\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            track_id = track.track_id\n",
        "            l,t,r,b = map(int, track.to_ltrb())\n",
        "            crop = frame[t:b, l:r]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop = cv2.resize(crop, img_size)\n",
        "            crop = crop / 255.0\n",
        "\n",
        "            if track_id not in tracks_dict:\n",
        "                tracks_dict[track_id] = []\n",
        "                tracks_preds[track_id] = []  # initialize predictions list\n",
        "            tracks_dict[track_id].append(crop)\n",
        "\n",
        "            # If enough frames collected, classify\n",
        "            if len(tracks_dict[track_id]) >= num_frames:\n",
        "                clip = np.array(tracks_dict[track_id][:num_frames])\n",
        "                clip = np.expand_dims(clip, axis=0)\n",
        "                pred = model.predict(clip)\n",
        "                pred_class = np.argmax(pred)\n",
        "                tracks_preds[track_id].append(pred_class)  # store prediction\n",
        "\n",
        "                # Keep overlap frames for smooth tracking\n",
        "                tracks_dict[track_id] = tracks_dict[track_id][8:]\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Compute final class per person using majority vote\n",
        "    from collections import Counter\n",
        "    final_classes = {}\n",
        "    for track_id, preds in tracks_preds.items():\n",
        "        if len(preds) == 0:\n",
        "            continue\n",
        "        most_common = Counter(preds).most_common(1)[0][0]\n",
        "        final_classes[track_id] = class_names[most_common]\n",
        "\n",
        "    # Print final results\n",
        "    for track_id, final_class in final_classes.items():\n",
        "        print(f\"Person {track_id} -> Final class: {final_class}\")\n",
        "\n",
        "    return final_classes  # optional return if needed for further use\n"
      ],
      "metadata": {
        "id": "s-kzlnNhkK6l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/convet_action.mp4\"\n",
        "process_video(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORaqCfdqce0u",
        "outputId": "ca2db6f0-060e-4ce8-bc93-9a8ec875b079"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 2 persons, 250.1ms\n",
            "Speed: 4.6ms preprocess, 250.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 231.4ms\n",
            "Speed: 4.2ms preprocess, 231.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 229.4ms\n",
            "Speed: 8.4ms preprocess, 229.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 229.0ms\n",
            "Speed: 3.7ms preprocess, 229.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 229.0ms\n",
            "Speed: 3.9ms preprocess, 229.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 236.6ms\n",
            "Speed: 3.9ms preprocess, 236.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 267.7ms\n",
            "Speed: 5.1ms preprocess, 267.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 148.5ms\n",
            "Speed: 3.7ms preprocess, 148.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 156.3ms\n",
            "Speed: 5.8ms preprocess, 156.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 151.9ms\n",
            "Speed: 3.9ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 160.2ms\n",
            "Speed: 4.4ms preprocess, 160.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 155.2ms\n",
            "Speed: 4.8ms preprocess, 155.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 160.3ms\n",
            "Speed: 4.3ms preprocess, 160.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 152.3ms\n",
            "Speed: 4.2ms preprocess, 152.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 162.5ms\n",
            "Speed: 3.3ms preprocess, 162.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 152.3ms\n",
            "Speed: 4.5ms preprocess, 152.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.0ms\n",
            "Speed: 5.7ms preprocess, 153.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.4ms\n",
            "Speed: 3.2ms preprocess, 146.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 162.9ms\n",
            "Speed: 4.7ms preprocess, 162.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 173.1ms\n",
            "Speed: 4.2ms preprocess, 173.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 151.2ms\n",
            "Speed: 4.7ms preprocess, 151.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 163.6ms\n",
            "Speed: 4.4ms preprocess, 163.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 154.0ms\n",
            "Speed: 3.6ms preprocess, 154.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.0ms\n",
            "Speed: 4.5ms preprocess, 160.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 1 person, 161.0ms\n",
            "Speed: 5.9ms preprocess, 161.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.8ms\n",
            "Speed: 4.6ms preprocess, 153.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.8ms\n",
            "Speed: 4.3ms preprocess, 147.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.6ms\n",
            "Speed: 4.0ms preprocess, 155.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 168.9ms\n",
            "Speed: 3.9ms preprocess, 168.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 145.3ms\n",
            "Speed: 4.5ms preprocess, 145.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.3ms\n",
            "Speed: 4.1ms preprocess, 153.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.8ms\n",
            "Speed: 4.3ms preprocess, 148.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 2 persons, 147.1ms\n",
            "Speed: 2.9ms preprocess, 147.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
            "\n",
            "0: 384x640 2 persons, 225.8ms\n",
            "Speed: 6.0ms preprocess, 225.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 220.6ms\n",
            "Speed: 3.8ms preprocess, 220.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 238.7ms\n",
            "Speed: 3.9ms preprocess, 238.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 222.3ms\n",
            "Speed: 5.3ms preprocess, 222.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 230.0ms\n",
            "Speed: 5.2ms preprocess, 230.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\n",
            "0: 384x640 3 persons, 154.2ms\n",
            "Speed: 3.6ms preprocess, 154.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.2ms\n",
            "Speed: 4.5ms preprocess, 150.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.6ms\n",
            "Speed: 4.7ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.5ms\n",
            "Speed: 4.6ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.2ms\n",
            "Speed: 3.4ms preprocess, 148.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 183.6ms\n",
            "Speed: 4.3ms preprocess, 183.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.0ms\n",
            "Speed: 3.9ms preprocess, 151.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.1ms\n",
            "Speed: 4.4ms preprocess, 152.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 156.3ms\n",
            "Speed: 4.0ms preprocess, 156.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.3ms\n",
            "Speed: 4.4ms preprocess, 148.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.7ms\n",
            "Speed: 5.7ms preprocess, 149.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.6ms\n",
            "Speed: 4.3ms preprocess, 149.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 2 persons, 180.4ms\n",
            "Speed: 6.4ms preprocess, 180.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.1ms\n",
            "Speed: 6.2ms preprocess, 150.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.8ms\n",
            "Speed: 5.8ms preprocess, 155.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 157.5ms\n",
            "Speed: 4.4ms preprocess, 157.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.4ms\n",
            "Speed: 4.4ms preprocess, 150.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.4ms\n",
            "Speed: 4.6ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 166.1ms\n",
            "Speed: 4.1ms preprocess, 166.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.8ms\n",
            "Speed: 5.1ms preprocess, 150.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 162.4ms\n",
            "Speed: 5.0ms preprocess, 162.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.2ms\n",
            "Speed: 5.1ms preprocess, 149.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.5ms\n",
            "Speed: 5.3ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 150.6ms\n",
            "Speed: 3.9ms preprocess, 150.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 158.8ms\n",
            "Speed: 3.9ms preprocess, 158.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 186.3ms\n",
            "Speed: 4.9ms preprocess, 186.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 234.9ms\n",
            "Speed: 4.3ms preprocess, 234.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
            "\n",
            "0: 384x640 1 person, 231.0ms\n",
            "Speed: 4.2ms preprocess, 231.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
            "\n",
            "0: 384x640 1 person, 233.7ms\n",
            "Speed: 9.6ms preprocess, 233.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 233.0ms\n",
            "Speed: 4.4ms preprocess, 233.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 235.5ms\n",
            "Speed: 7.0ms preprocess, 235.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\n",
            "0: 384x640 1 person, 171.4ms\n",
            "Speed: 5.3ms preprocess, 171.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.9ms\n",
            "Speed: 3.7ms preprocess, 152.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.5ms\n",
            "Speed: 3.8ms preprocess, 155.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.2ms\n",
            "Speed: 3.3ms preprocess, 146.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.9ms\n",
            "Speed: 5.6ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 154.7ms\n",
            "Speed: 4.2ms preprocess, 154.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 172.9ms\n",
            "Speed: 4.7ms preprocess, 172.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 154.6ms\n",
            "Speed: 5.3ms preprocess, 154.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 (no detections), 150.7ms\n",
            "Speed: 4.2ms preprocess, 150.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 170.0ms\n",
            "Speed: 3.9ms preprocess, 170.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.2ms\n",
            "Speed: 4.3ms preprocess, 153.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 149.6ms\n",
            "Speed: 4.6ms preprocess, 149.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 2 persons, 152.8ms\n",
            "Speed: 4.2ms preprocess, 152.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.8ms\n",
            "Speed: 4.2ms preprocess, 147.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 149.2ms\n",
            "Speed: 4.4ms preprocess, 149.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 145.7ms\n",
            "Speed: 4.0ms preprocess, 145.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 4 persons, 171.6ms\n",
            "Speed: 5.6ms preprocess, 171.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 147.2ms\n",
            "Speed: 3.8ms preprocess, 147.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 172.5ms\n",
            "Speed: 5.5ms preprocess, 172.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 149.4ms\n",
            "Speed: 4.0ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 174.0ms\n",
            "Speed: 4.4ms preprocess, 174.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 149.3ms\n",
            "Speed: 5.5ms preprocess, 149.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 151.9ms\n",
            "Speed: 5.3ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 243.7ms\n",
            "Speed: 6.0ms preprocess, 243.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
            "\n",
            "0: 384x640 6 persons, 253.1ms\n",
            "Speed: 8.1ms preprocess, 253.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 229.2ms\n",
            "Speed: 4.1ms preprocess, 229.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 236.1ms\n",
            "Speed: 4.7ms preprocess, 236.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 175.2ms\n",
            "Speed: 11.2ms preprocess, 175.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.3ms\n",
            "Speed: 4.6ms preprocess, 148.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.5ms\n",
            "Speed: 4.5ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 173.8ms\n",
            "Speed: 4.7ms preprocess, 173.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 146.3ms\n",
            "Speed: 4.6ms preprocess, 146.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 384x640 7 persons, 152.0ms\n",
            "Speed: 4.6ms preprocess, 152.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 157.5ms\n",
            "Speed: 4.3ms preprocess, 157.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 174.9ms\n",
            "Speed: 5.6ms preprocess, 174.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 384x640 8 persons, 173.5ms\n",
            "Speed: 4.6ms preprocess, 173.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 168.2ms\n",
            "Speed: 3.2ms preprocess, 168.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 5 persons, 155.3ms\n",
            "Speed: 4.6ms preprocess, 155.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 150.4ms\n",
            "Speed: 4.3ms preprocess, 150.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 5 persons, 168.1ms\n",
            "Speed: 3.9ms preprocess, 168.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
            "\n",
            "0: 384x640 6 persons, 268.5ms\n",
            "Speed: 12.5ms preprocess, 268.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 234.7ms\n",
            "Speed: 4.5ms preprocess, 234.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 244.3ms\n",
            "Speed: 6.5ms preprocess, 244.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
            "\n",
            "0: 384x640 6 persons, 183.5ms\n",
            "Speed: 4.1ms preprocess, 183.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.9ms\n",
            "Speed: 3.4ms preprocess, 151.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\n",
            "0: 384x640 3 persons, 151.7ms\n",
            "Speed: 3.5ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 153.8ms\n",
            "Speed: 4.0ms preprocess, 153.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 384x640 4 persons, 154.8ms\n",
            "Speed: 5.5ms preprocess, 154.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 4 persons, 165.2ms\n",
            "Speed: 4.9ms preprocess, 165.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\n",
            "0: 384x640 5 persons, 171.6ms\n",
            "Speed: 4.4ms preprocess, 171.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 156.5ms\n",
            "Speed: 7.1ms preprocess, 156.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 4 persons, 157.6ms\n",
            "Speed: 6.7ms preprocess, 157.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 (no detections), 153.8ms\n",
            "Speed: 5.1ms preprocess, 153.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.1ms\n",
            "Speed: 5.2ms preprocess, 150.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.4ms\n",
            "Speed: 5.7ms preprocess, 148.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\n",
            "0: 384x640 1 person, 175.1ms\n",
            "Speed: 7.3ms preprocess, 175.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.8ms\n",
            "Speed: 5.2ms preprocess, 153.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 4 persons, 156.3ms\n",
            "Speed: 5.4ms preprocess, 156.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\n",
            "0: 384x640 3 persons, 231.6ms\n",
            "Speed: 4.1ms preprocess, 231.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 252.6ms\n",
            "Speed: 5.5ms preprocess, 252.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
            "\n",
            "0: 384x640 2 persons, 245.8ms\n",
            "Speed: 3.9ms preprocess, 245.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
            "\n",
            "0: 384x640 1 person, 147.2ms\n",
            "Speed: 3.8ms preprocess, 147.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.7ms\n",
            "Speed: 4.2ms preprocess, 149.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 (no detections), 170.1ms\n",
            "Speed: 8.6ms preprocess, 170.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
            "\n",
            "0: 384x640 4 persons, 158.8ms\n",
            "Speed: 4.1ms preprocess, 158.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 4 persons, 153.8ms\n",
            "Speed: 5.6ms preprocess, 153.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 7 persons, 148.0ms\n",
            "Speed: 4.3ms preprocess, 148.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 5 persons, 157.8ms\n",
            "Speed: 4.1ms preprocess, 157.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 5 persons, 148.6ms\n",
            "Speed: 4.6ms preprocess, 148.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
            "\n",
            "0: 384x640 2 persons, 154.0ms\n",
            "Speed: 5.0ms preprocess, 154.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 157.6ms\n",
            "Speed: 5.4ms preprocess, 157.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 167.4ms\n",
            "Speed: 4.1ms preprocess, 167.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 1 person, 166.8ms\n",
            "Speed: 4.4ms preprocess, 166.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 6 persons, 153.0ms\n",
            "Speed: 4.2ms preprocess, 153.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
            "\n",
            "0: 384x640 4 persons, 149.4ms\n",
            "Speed: 3.3ms preprocess, 149.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
            "\n",
            "0: 384x640 7 persons, 225.2ms\n",
            "Speed: 4.1ms preprocess, 225.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
            "\n",
            "0: 384x640 4 persons, 255.5ms\n",
            "Speed: 8.8ms preprocess, 255.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
            "\n",
            "0: 384x640 6 persons, 164.9ms\n",
            "Speed: 5.8ms preprocess, 164.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 150.6ms\n",
            "Speed: 4.0ms preprocess, 150.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 3 persons, 156.0ms\n",
            "Speed: 5.0ms preprocess, 156.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 4 persons, 152.7ms\n",
            "Speed: 3.1ms preprocess, 152.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 4 persons, 151.6ms\n",
            "Speed: 5.4ms preprocess, 151.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 3 persons, 153.6ms\n",
            "Speed: 5.6ms preprocess, 153.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 3 persons, 147.9ms\n",
            "Speed: 4.0ms preprocess, 147.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 (no detections), 146.4ms\n",
            "Speed: 7.5ms preprocess, 146.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.9ms\n",
            "Speed: 4.5ms preprocess, 153.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 1 person, 157.5ms\n",
            "Speed: 5.2ms preprocess, 157.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.3ms\n",
            "Speed: 4.5ms preprocess, 150.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.1ms\n",
            "Speed: 8.7ms preprocess, 151.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 2 persons, 148.0ms\n",
            "Speed: 4.5ms preprocess, 148.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 1 person, 165.3ms\n",
            "Speed: 4.6ms preprocess, 165.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.2ms\n",
            "Speed: 4.3ms preprocess, 153.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
            "\n",
            "0: 384x640 1 person, 228.4ms\n",
            "Speed: 9.1ms preprocess, 228.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
            "\n",
            "0: 384x640 1 person, 240.8ms\n",
            "Speed: 4.9ms preprocess, 240.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
            "\n",
            "0: 384x640 1 person, 231.0ms\n",
            "Speed: 5.3ms preprocess, 231.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 241.4ms\n",
            "Speed: 7.3ms preprocess, 241.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 239.3ms\n",
            "Speed: 7.3ms preprocess, 239.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.8ms\n",
            "Speed: 3.0ms preprocess, 148.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 1 person, 158.0ms\n",
            "Speed: 5.2ms preprocess, 158.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.6ms\n",
            "Speed: 3.0ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 3 persons, 151.9ms\n",
            "Speed: 6.7ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.4ms\n",
            "Speed: 5.2ms preprocess, 151.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 154.0ms\n",
            "Speed: 4.4ms preprocess, 154.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\n",
            "0: 384x640 3 persons, 155.5ms\n",
            "Speed: 4.8ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.1ms\n",
            "Speed: 4.2ms preprocess, 153.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 168.1ms\n",
            "Speed: 5.0ms preprocess, 168.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\n",
            "0: 384x640 2 persons, 158.1ms\n",
            "Speed: 4.7ms preprocess, 158.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 161.8ms\n",
            "Speed: 4.3ms preprocess, 161.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.2ms\n",
            "Speed: 4.0ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 151.8ms\n",
            "Speed: 4.2ms preprocess, 151.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 167.4ms\n",
            "Speed: 4.5ms preprocess, 167.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 (no detections), 184.5ms\n",
            "Speed: 6.7ms preprocess, 184.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 2 persons, 168.0ms\n",
            "Speed: 4.6ms preprocess, 168.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 3 persons, 149.9ms\n",
            "Speed: 5.3ms preprocess, 149.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 1 person, 146.8ms\n",
            "Speed: 5.1ms preprocess, 146.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.1ms\n",
            "Speed: 5.2ms preprocess, 149.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 224.4ms\n",
            "Speed: 3.9ms preprocess, 224.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 227.9ms\n",
            "Speed: 10.4ms preprocess, 227.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 236.7ms\n",
            "Speed: 5.4ms preprocess, 236.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
            "\n",
            "0: 384x640 2 persons, 221.9ms\n",
            "Speed: 4.1ms preprocess, 221.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 253.9ms\n",
            "Speed: 7.7ms preprocess, 253.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.6ms\n",
            "Speed: 4.1ms preprocess, 154.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.9ms\n",
            "Speed: 5.4ms preprocess, 149.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.1ms\n",
            "Speed: 4.7ms preprocess, 151.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.2ms\n",
            "Speed: 4.4ms preprocess, 149.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.3ms\n",
            "Speed: 4.5ms preprocess, 155.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.3ms\n",
            "Speed: 4.4ms preprocess, 152.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.9ms\n",
            "Speed: 4.5ms preprocess, 150.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 154.1ms\n",
            "Speed: 4.0ms preprocess, 154.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.9ms\n",
            "Speed: 4.3ms preprocess, 150.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 384x640 1 person, 160.2ms\n",
            "Speed: 3.5ms preprocess, 160.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.0ms\n",
            "Speed: 4.1ms preprocess, 152.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 145.4ms\n",
            "Speed: 4.7ms preprocess, 145.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.1ms\n",
            "Speed: 4.0ms preprocess, 148.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 145.9ms\n",
            "Speed: 3.7ms preprocess, 145.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 171.9ms\n",
            "Speed: 3.8ms preprocess, 171.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.5ms\n",
            "Speed: 3.9ms preprocess, 149.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.5ms\n",
            "Speed: 5.1ms preprocess, 150.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 2 persons, 154.2ms\n",
            "Speed: 4.1ms preprocess, 154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.0ms\n",
            "Speed: 8.4ms preprocess, 147.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.5ms\n",
            "Speed: 3.7ms preprocess, 152.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 1 person, 150.0ms\n",
            "Speed: 4.2ms preprocess, 150.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 155.6ms\n",
            "Speed: 4.1ms preprocess, 155.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.5ms\n",
            "Speed: 5.2ms preprocess, 149.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.1ms\n",
            "Speed: 7.8ms preprocess, 147.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 235.6ms\n",
            "Speed: 4.1ms preprocess, 235.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
            "\n",
            "0: 384x640 3 persons, 238.5ms\n",
            "Speed: 4.1ms preprocess, 238.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 230.1ms\n",
            "Speed: 4.3ms preprocess, 230.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 248.9ms\n",
            "Speed: 7.7ms preprocess, 248.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
            "\n",
            "0: 384x640 2 persons, 146.7ms\n",
            "Speed: 7.1ms preprocess, 146.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.1ms\n",
            "Speed: 3.6ms preprocess, 148.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.7ms\n",
            "Speed: 4.2ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.7ms\n",
            "Speed: 5.6ms preprocess, 148.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 159.7ms\n",
            "Speed: 4.5ms preprocess, 159.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 2 persons, 165.5ms\n",
            "Speed: 5.2ms preprocess, 165.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.6ms\n",
            "Speed: 3.0ms preprocess, 151.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 150.2ms\n",
            "Speed: 4.5ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 148.9ms\n",
            "Speed: 4.0ms preprocess, 148.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 156.0ms\n",
            "Speed: 5.3ms preprocess, 156.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 165.1ms\n",
            "Speed: 4.3ms preprocess, 165.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 2 persons, 150.9ms\n",
            "Speed: 4.6ms preprocess, 150.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 166.3ms\n",
            "Speed: 6.4ms preprocess, 166.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\n",
            "0: 384x640 2 persons, 154.2ms\n",
            "Speed: 4.6ms preprocess, 154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 158.2ms\n",
            "Speed: 4.5ms preprocess, 158.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 168.3ms\n",
            "Speed: 12.5ms preprocess, 168.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.7ms\n",
            "Speed: 4.0ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 175.5ms\n",
            "Speed: 6.1ms preprocess, 175.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 1 person, 161.9ms\n",
            "Speed: 5.1ms preprocess, 161.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.9ms\n",
            "Speed: 3.1ms preprocess, 150.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.3ms\n",
            "Speed: 4.1ms preprocess, 152.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.8ms\n",
            "Speed: 4.0ms preprocess, 152.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.2ms\n",
            "Speed: 7.7ms preprocess, 160.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 242.1ms\n",
            "Speed: 5.5ms preprocess, 242.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 247.8ms\n",
            "Speed: 5.2ms preprocess, 247.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 222.2ms\n",
            "Speed: 5.5ms preprocess, 222.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
            "\n",
            "0: 384x640 2 persons, 226.3ms\n",
            "Speed: 7.4ms preprocess, 226.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 226.1ms\n",
            "Speed: 4.8ms preprocess, 226.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 240.0ms\n",
            "Speed: 5.4ms preprocess, 240.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 157.1ms\n",
            "Speed: 6.6ms preprocess, 157.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 148.2ms\n",
            "Speed: 4.6ms preprocess, 148.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.6ms\n",
            "Speed: 4.1ms preprocess, 151.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 172.1ms\n",
            "Speed: 4.9ms preprocess, 172.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 149.0ms\n",
            "Speed: 4.8ms preprocess, 149.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 384x640 3 persons, 169.2ms\n",
            "Speed: 3.8ms preprocess, 169.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 155.9ms\n",
            "Speed: 4.4ms preprocess, 155.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 152.7ms\n",
            "Speed: 4.4ms preprocess, 152.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.5ms\n",
            "Speed: 3.6ms preprocess, 150.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.7ms\n",
            "Speed: 5.4ms preprocess, 147.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 164.1ms\n",
            "Speed: 9.5ms preprocess, 164.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.5ms\n",
            "Speed: 4.1ms preprocess, 149.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.9ms\n",
            "Speed: 6.2ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.2ms\n",
            "Speed: 4.4ms preprocess, 153.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.2ms\n",
            "Speed: 4.4ms preprocess, 153.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 162.1ms\n",
            "Speed: 4.9ms preprocess, 162.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 157.6ms\n",
            "Speed: 4.8ms preprocess, 157.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 145.2ms\n",
            "Speed: 4.9ms preprocess, 145.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 155.0ms\n",
            "Speed: 5.8ms preprocess, 155.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 2 persons, 148.1ms\n",
            "Speed: 4.4ms preprocess, 148.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 156.8ms\n",
            "Speed: 3.8ms preprocess, 156.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 2 persons, 150.8ms\n",
            "Speed: 4.4ms preprocess, 150.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 179.6ms\n",
            "Speed: 4.7ms preprocess, 179.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 245.1ms\n",
            "Speed: 9.2ms preprocess, 245.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 235.2ms\n",
            "Speed: 12.0ms preprocess, 235.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 234.1ms\n",
            "Speed: 3.8ms preprocess, 234.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 237.2ms\n",
            "Speed: 4.3ms preprocess, 237.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
            "\n",
            "0: 384x640 7 persons, 265.5ms\n",
            "Speed: 10.3ms preprocess, 265.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 150.9ms\n",
            "Speed: 4.5ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 4 persons, 157.5ms\n",
            "Speed: 4.2ms preprocess, 157.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 151.5ms\n",
            "Speed: 6.4ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 148.8ms\n",
            "Speed: 5.7ms preprocess, 148.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 155.6ms\n",
            "Speed: 7.1ms preprocess, 155.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 165.2ms\n",
            "Speed: 7.8ms preprocess, 165.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 157.7ms\n",
            "Speed: 5.0ms preprocess, 157.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 3 persons, 157.3ms\n",
            "Speed: 7.2ms preprocess, 157.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 169.2ms\n",
            "Speed: 4.1ms preprocess, 169.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\n",
            "0: 384x640 3 persons, 153.4ms\n",
            "Speed: 4.2ms preprocess, 153.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 161.3ms\n",
            "Speed: 4.0ms preprocess, 161.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 161.4ms\n",
            "Speed: 10.9ms preprocess, 161.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 152.8ms\n",
            "Speed: 7.3ms preprocess, 152.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.8ms\n",
            "Speed: 13.5ms preprocess, 149.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.5ms\n",
            "Speed: 6.2ms preprocess, 153.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 2 persons, 155.1ms\n",
            "Speed: 4.4ms preprocess, 155.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\n",
            "0: 384x640 2 persons, 269.1ms\n",
            "Speed: 10.0ms preprocess, 269.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "\n",
            "0: 384x640 2 persons, 240.0ms\n",
            "Speed: 7.0ms preprocess, 240.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 225.5ms\n",
            "Speed: 6.0ms preprocess, 225.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 220.1ms\n",
            "Speed: 4.2ms preprocess, 220.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 236.1ms\n",
            "Speed: 4.0ms preprocess, 236.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 164.7ms\n",
            "Speed: 4.1ms preprocess, 164.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.7ms\n",
            "Speed: 3.5ms preprocess, 152.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.8ms\n",
            "Speed: 6.2ms preprocess, 154.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.2ms\n",
            "Speed: 4.3ms preprocess, 147.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.0ms\n",
            "Speed: 5.2ms preprocess, 149.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 154.9ms\n",
            "Speed: 8.2ms preprocess, 154.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 155.5ms\n",
            "Speed: 4.1ms preprocess, 155.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "\n",
            "0: 384x640 2 persons, 150.0ms\n",
            "Speed: 5.4ms preprocess, 150.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
            "\n",
            "0: 384x640 2 persons, 155.1ms\n",
            "Speed: 5.5ms preprocess, 155.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.5ms\n",
            "Speed: 4.7ms preprocess, 149.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.8ms\n",
            "Speed: 4.2ms preprocess, 151.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.3ms\n",
            "Speed: 5.3ms preprocess, 149.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 2 persons, 152.0ms\n",
            "Speed: 5.4ms preprocess, 152.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 154.5ms\n",
            "Speed: 6.5ms preprocess, 154.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.1ms\n",
            "Speed: 4.4ms preprocess, 153.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.6ms\n",
            "Speed: 4.5ms preprocess, 149.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.7ms\n",
            "Speed: 4.8ms preprocess, 153.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.1ms\n",
            "Speed: 4.0ms preprocess, 153.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 1 person, 145.0ms\n",
            "Speed: 5.3ms preprocess, 145.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.1ms\n",
            "Speed: 4.4ms preprocess, 153.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\n",
            "0: 384x640 1 person, 153.0ms\n",
            "Speed: 4.9ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.2ms\n",
            "Speed: 4.7ms preprocess, 151.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 227.1ms\n",
            "Speed: 5.3ms preprocess, 227.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 228.6ms\n",
            "Speed: 16.2ms preprocess, 228.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 234.9ms\n",
            "Speed: 5.2ms preprocess, 234.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 228.8ms\n",
            "Speed: 8.9ms preprocess, 228.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "\n",
            "0: 384x640 2 persons, 236.5ms\n",
            "Speed: 8.7ms preprocess, 236.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 224.6ms\n",
            "Speed: 9.8ms preprocess, 224.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\n",
            "0: 384x640 3 persons, 146.5ms\n",
            "Speed: 4.7ms preprocess, 146.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 171.1ms\n",
            "Speed: 4.2ms preprocess, 171.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 151.5ms\n",
            "Speed: 4.9ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 174.6ms\n",
            "Speed: 3.9ms preprocess, 174.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 146.0ms\n",
            "Speed: 4.9ms preprocess, 146.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 169.7ms\n",
            "Speed: 4.9ms preprocess, 169.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\n",
            "0: 384x640 2 persons, 146.3ms\n",
            "Speed: 5.2ms preprocess, 146.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 170.9ms\n",
            "Speed: 5.3ms preprocess, 170.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.5ms\n",
            "Speed: 4.9ms preprocess, 149.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 176.5ms\n",
            "Speed: 4.0ms preprocess, 176.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.5ms\n",
            "Speed: 5.8ms preprocess, 150.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 155.3ms\n",
            "Speed: 6.8ms preprocess, 155.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 164.7ms\n",
            "Speed: 4.2ms preprocess, 164.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 177.3ms\n",
            "Speed: 5.2ms preprocess, 177.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 4 persons, 151.7ms\n",
            "Speed: 5.3ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 147.7ms\n",
            "Speed: 5.0ms preprocess, 147.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 4 persons, 187.9ms\n",
            "Speed: 7.0ms preprocess, 187.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 236.3ms\n",
            "Speed: 4.1ms preprocess, 236.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\n",
            "0: 384x640 1 person, 231.7ms\n",
            "Speed: 9.0ms preprocess, 231.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 228.4ms\n",
            "Speed: 6.4ms preprocess, 228.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 235.1ms\n",
            "Speed: 5.4ms preprocess, 235.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
            "\n",
            "0: 384x640 1 person, 200.5ms\n",
            "Speed: 8.8ms preprocess, 200.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.2ms\n",
            "Speed: 4.8ms preprocess, 148.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 167.5ms\n",
            "Speed: 4.1ms preprocess, 167.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\n",
            "0: 384x640 3 persons, 167.0ms\n",
            "Speed: 3.2ms preprocess, 167.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 155.8ms\n",
            "Speed: 3.9ms preprocess, 155.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 3 persons, 175.0ms\n",
            "Speed: 4.4ms preprocess, 175.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 384x640 3 persons, 157.6ms\n",
            "Speed: 4.8ms preprocess, 157.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 147.4ms\n",
            "Speed: 7.0ms preprocess, 147.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 384x640 4 persons, 174.7ms\n",
            "Speed: 4.1ms preprocess, 174.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\n",
            "0: 384x640 3 persons, 154.7ms\n",
            "Speed: 4.4ms preprocess, 154.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 4 persons, 149.4ms\n",
            "Speed: 4.0ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 4 persons, 153.6ms\n",
            "Speed: 4.2ms preprocess, 153.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 166.1ms\n",
            "Speed: 8.6ms preprocess, 166.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 147.8ms\n",
            "Speed: 6.7ms preprocess, 147.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 384x640 1 person, 160.1ms\n",
            "Speed: 4.4ms preprocess, 160.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "\n",
            "0: 384x640 1 person, 260.1ms\n",
            "Speed: 4.4ms preprocess, 260.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "\n",
            "0: 384x640 1 person, 233.4ms\n",
            "Speed: 6.6ms preprocess, 233.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
            "\n",
            "0: 384x640 1 person, 231.0ms\n",
            "Speed: 6.9ms preprocess, 231.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 264.2ms\n",
            "Speed: 8.9ms preprocess, 264.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "\n",
            "0: 384x640 1 person, 244.5ms\n",
            "Speed: 7.6ms preprocess, 244.5ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 188.0ms\n",
            "Speed: 4.0ms preprocess, 188.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 151.3ms\n",
            "Speed: 3.5ms preprocess, 151.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "\n",
            "0: 384x640 3 persons, 151.2ms\n",
            "Speed: 4.2ms preprocess, 151.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 150.9ms\n",
            "Speed: 10.9ms preprocess, 150.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 148.2ms\n",
            "Speed: 3.0ms preprocess, 148.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 4 persons, 169.4ms\n",
            "Speed: 3.5ms preprocess, 169.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 3 persons, 150.9ms\n",
            "Speed: 4.1ms preprocess, 150.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 3 persons, 150.4ms\n",
            "Speed: 5.0ms preprocess, 150.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\n",
            "0: 384x640 3 persons, 169.2ms\n",
            "Speed: 4.0ms preprocess, 169.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 153.6ms\n",
            "Speed: 6.7ms preprocess, 153.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 147.4ms\n",
            "Speed: 4.6ms preprocess, 147.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 152.9ms\n",
            "Speed: 3.9ms preprocess, 152.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 147.3ms\n",
            "Speed: 6.3ms preprocess, 147.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 3 persons, 152.2ms\n",
            "Speed: 4.2ms preprocess, 152.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 3 persons, 149.0ms\n",
            "Speed: 3.8ms preprocess, 149.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.0ms\n",
            "Speed: 8.3ms preprocess, 149.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.6ms\n",
            "Speed: 3.6ms preprocess, 147.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 174.0ms\n",
            "Speed: 3.0ms preprocess, 174.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 240.0ms\n",
            "Speed: 6.9ms preprocess, 240.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "\n",
            "0: 384x640 1 person, 284.4ms\n",
            "Speed: 7.3ms preprocess, 284.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 233.8ms\n",
            "Speed: 3.9ms preprocess, 233.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "\n",
            "0: 384x640 1 person, 244.3ms\n",
            "Speed: 8.1ms preprocess, 244.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.5ms\n",
            "Speed: 4.1ms preprocess, 153.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.6ms\n",
            "Speed: 4.3ms preprocess, 151.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 181.5ms\n",
            "Speed: 6.5ms preprocess, 181.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 152.8ms\n",
            "Speed: 3.9ms preprocess, 152.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 157.4ms\n",
            "Speed: 3.9ms preprocess, 157.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.6ms\n",
            "Speed: 5.0ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 154.2ms\n",
            "Speed: 7.9ms preprocess, 154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.8ms\n",
            "Speed: 6.7ms preprocess, 154.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.7ms\n",
            "Speed: 4.3ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.0ms\n",
            "Speed: 4.7ms preprocess, 152.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.5ms\n",
            "Speed: 3.5ms preprocess, 150.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.9ms\n",
            "Speed: 3.8ms preprocess, 150.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.4ms\n",
            "Speed: 4.9ms preprocess, 148.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 3 persons, 156.6ms\n",
            "Speed: 4.4ms preprocess, 156.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 160.9ms\n",
            "Speed: 5.9ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 3 persons, 156.7ms\n",
            "Speed: 4.0ms preprocess, 156.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.1ms\n",
            "Speed: 6.3ms preprocess, 151.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.0ms\n",
            "Speed: 5.4ms preprocess, 152.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.7ms\n",
            "Speed: 4.1ms preprocess, 153.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 150.5ms\n",
            "Speed: 5.0ms preprocess, 150.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.3ms\n",
            "Speed: 5.8ms preprocess, 151.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 3 persons, 161.4ms\n",
            "Speed: 4.6ms preprocess, 161.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 272.0ms\n",
            "Speed: 4.9ms preprocess, 272.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\n",
            "0: 384x640 1 person, 234.7ms\n",
            "Speed: 6.4ms preprocess, 234.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 229.6ms\n",
            "Speed: 5.9ms preprocess, 229.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
            "\n",
            "0: 384x640 3 persons, 245.8ms\n",
            "Speed: 5.3ms preprocess, 245.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 238.9ms\n",
            "Speed: 5.7ms preprocess, 238.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 156.9ms\n",
            "Speed: 4.9ms preprocess, 156.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 154.7ms\n",
            "Speed: 4.4ms preprocess, 154.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 154.5ms\n",
            "Speed: 4.2ms preprocess, 154.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.5ms\n",
            "Speed: 5.4ms preprocess, 153.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 2 persons, 152.6ms\n",
            "Speed: 5.0ms preprocess, 152.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 155.4ms\n",
            "Speed: 6.0ms preprocess, 155.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.4ms\n",
            "Speed: 6.9ms preprocess, 149.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.9ms\n",
            "Speed: 4.6ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 164.3ms\n",
            "Speed: 8.8ms preprocess, 164.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.1ms\n",
            "Speed: 4.0ms preprocess, 152.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 2 persons, 159.3ms\n",
            "Speed: 4.9ms preprocess, 159.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.7ms\n",
            "Speed: 3.9ms preprocess, 153.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 3 persons, 149.9ms\n",
            "Speed: 6.0ms preprocess, 149.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.7ms\n",
            "Speed: 4.7ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 3 persons, 148.2ms\n",
            "Speed: 4.6ms preprocess, 148.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 152.0ms\n",
            "Speed: 4.3ms preprocess, 152.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 151.9ms\n",
            "Speed: 4.3ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 239.3ms\n",
            "Speed: 4.8ms preprocess, 239.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
            "\n",
            "0: 384x640 5 persons, 233.3ms\n",
            "Speed: 3.9ms preprocess, 233.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 242.3ms\n",
            "Speed: 4.3ms preprocess, 242.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 2 persons, 151.4ms\n",
            "Speed: 3.4ms preprocess, 151.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 151.2ms\n",
            "Speed: 3.4ms preprocess, 151.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 2 persons, 150.3ms\n",
            "Speed: 5.8ms preprocess, 150.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 153.9ms\n",
            "Speed: 4.3ms preprocess, 153.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 180.4ms\n",
            "Speed: 10.3ms preprocess, 180.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.8ms\n",
            "Speed: 4.5ms preprocess, 153.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.8ms\n",
            "Speed: 8.1ms preprocess, 154.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.8ms\n",
            "Speed: 4.2ms preprocess, 148.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 384x640 2 persons, 157.5ms\n",
            "Speed: 4.5ms preprocess, 157.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 154.7ms\n",
            "Speed: 5.2ms preprocess, 154.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.6ms\n",
            "Speed: 4.8ms preprocess, 146.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.4ms\n",
            "Speed: 7.7ms preprocess, 151.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.0ms\n",
            "Speed: 3.8ms preprocess, 155.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 (no detections), 174.0ms\n",
            "Speed: 4.0ms preprocess, 174.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 154.2ms\n",
            "Speed: 6.0ms preprocess, 154.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 154.5ms\n",
            "Speed: 7.3ms preprocess, 154.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.5ms\n",
            "Speed: 3.9ms preprocess, 146.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.8ms\n",
            "Speed: 7.2ms preprocess, 147.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.1ms\n",
            "Speed: 4.1ms preprocess, 153.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.7ms\n",
            "Speed: 3.4ms preprocess, 149.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 240.4ms\n",
            "Speed: 6.0ms preprocess, 240.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
            "\n",
            "0: 384x640 (no detections), 245.3ms\n",
            "Speed: 4.4ms preprocess, 245.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 235.3ms\n",
            "Speed: 11.2ms preprocess, 235.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 246.3ms\n",
            "Speed: 7.2ms preprocess, 246.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 237.2ms\n",
            "Speed: 4.7ms preprocess, 237.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 156.0ms\n",
            "Speed: 4.1ms preprocess, 156.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 153.4ms\n",
            "Speed: 6.4ms preprocess, 153.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 157.2ms\n",
            "Speed: 9.6ms preprocess, 157.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 169.3ms\n",
            "Speed: 8.1ms preprocess, 169.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 2 persons, 174.1ms\n",
            "Speed: 4.2ms preprocess, 174.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 152.2ms\n",
            "Speed: 7.3ms preprocess, 152.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 148.4ms\n",
            "Speed: 9.0ms preprocess, 148.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 384x640 4 persons, 153.7ms\n",
            "Speed: 5.7ms preprocess, 153.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 162.6ms\n",
            "Speed: 4.0ms preprocess, 162.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 154.1ms\n",
            "Speed: 5.3ms preprocess, 154.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 158.2ms\n",
            "Speed: 4.4ms preprocess, 158.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 384x640 3 persons, 408.9ms\n",
            "Speed: 7.0ms preprocess, 408.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 149.4ms\n",
            "Speed: 7.1ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 150.8ms\n",
            "Speed: 5.3ms preprocess, 150.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 153.6ms\n",
            "Speed: 4.5ms preprocess, 153.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 167.3ms\n",
            "Speed: 6.3ms preprocess, 167.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 156.9ms\n",
            "Speed: 5.4ms preprocess, 156.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
            "\n",
            "0: 384x640 4 persons, 224.7ms\n",
            "Speed: 8.3ms preprocess, 224.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 228.5ms\n",
            "Speed: 4.3ms preprocess, 228.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
            "\n",
            "0: 384x640 3 persons, 242.8ms\n",
            "Speed: 7.8ms preprocess, 242.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "\n",
            "0: 384x640 3 persons, 155.9ms\n",
            "Speed: 5.1ms preprocess, 155.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 175.9ms\n",
            "Speed: 4.6ms preprocess, 175.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.3ms\n",
            "Speed: 4.9ms preprocess, 151.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 152.3ms\n",
            "Speed: 4.2ms preprocess, 152.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.7ms\n",
            "Speed: 4.3ms preprocess, 155.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 3 persons, 171.5ms\n",
            "Speed: 5.9ms preprocess, 171.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 161.4ms\n",
            "Speed: 4.0ms preprocess, 161.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 384x640 3 persons, 170.0ms\n",
            "Speed: 4.3ms preprocess, 170.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\n",
            "0: 384x640 5 persons, 154.3ms\n",
            "Speed: 7.3ms preprocess, 154.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 149.4ms\n",
            "Speed: 4.0ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "\n",
            "0: 384x640 4 persons, 159.2ms\n",
            "Speed: 4.3ms preprocess, 159.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 148.6ms\n",
            "Speed: 4.2ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.0ms\n",
            "Speed: 4.9ms preprocess, 154.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.0ms\n",
            "Speed: 4.8ms preprocess, 149.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 157.2ms\n",
            "Speed: 5.1ms preprocess, 157.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 5 persons, 160.3ms\n",
            "Speed: 6.5ms preprocess, 160.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 229.3ms\n",
            "Speed: 5.3ms preprocess, 229.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 227.1ms\n",
            "Speed: 5.5ms preprocess, 227.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
            "\n",
            "0: 384x640 1 person, 245.2ms\n",
            "Speed: 10.2ms preprocess, 245.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 225.7ms\n",
            "Speed: 8.9ms preprocess, 225.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
            "\n",
            "0: 384x640 3 persons, 270.7ms\n",
            "Speed: 12.3ms preprocess, 270.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.9ms\n",
            "Speed: 3.8ms preprocess, 150.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.3ms\n",
            "Speed: 8.4ms preprocess, 151.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.4ms\n",
            "Speed: 5.1ms preprocess, 154.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.0ms\n",
            "Speed: 4.2ms preprocess, 152.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.1ms\n",
            "Speed: 5.4ms preprocess, 160.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 (no detections), 148.9ms\n",
            "Speed: 5.4ms preprocess, 148.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 170.8ms\n",
            "Speed: 6.6ms preprocess, 170.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.8ms\n",
            "Speed: 5.4ms preprocess, 150.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 152.4ms\n",
            "Speed: 5.1ms preprocess, 152.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.6ms\n",
            "Speed: 5.0ms preprocess, 152.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 2 persons, 150.7ms\n",
            "Speed: 7.9ms preprocess, 150.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.7ms\n",
            "Speed: 3.8ms preprocess, 149.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.9ms\n",
            "Speed: 4.2ms preprocess, 153.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 384x640 2 persons, 149.0ms\n",
            "Speed: 5.5ms preprocess, 149.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.7ms\n",
            "Speed: 4.8ms preprocess, 153.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 162.4ms\n",
            "Speed: 4.5ms preprocess, 162.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.0ms\n",
            "Speed: 4.6ms preprocess, 151.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 2 persons, 172.7ms\n",
            "Speed: 8.4ms preprocess, 172.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\n",
            "0: 384x640 1 person, 158.2ms\n",
            "Speed: 3.9ms preprocess, 158.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.4ms\n",
            "Speed: 5.6ms preprocess, 149.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 159.2ms\n",
            "Speed: 6.0ms preprocess, 159.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 1 person, 156.3ms\n",
            "Speed: 4.1ms preprocess, 156.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 239.4ms\n",
            "Speed: 4.1ms preprocess, 239.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 235.1ms\n",
            "Speed: 4.5ms preprocess, 235.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\n",
            "0: 384x640 1 person, 240.3ms\n",
            "Speed: 10.7ms preprocess, 240.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "\n",
            "0: 384x640 1 person, 250.9ms\n",
            "Speed: 6.2ms preprocess, 250.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 230.9ms\n",
            "Speed: 5.3ms preprocess, 230.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 237.0ms\n",
            "Speed: 5.3ms preprocess, 237.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.0ms\n",
            "Speed: 6.2ms preprocess, 149.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.4ms\n",
            "Speed: 5.0ms preprocess, 150.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 171.6ms\n",
            "Speed: 4.1ms preprocess, 171.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.9ms\n",
            "Speed: 8.8ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.8ms\n",
            "Speed: 9.4ms preprocess, 149.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 2 persons, 154.1ms\n",
            "Speed: 4.4ms preprocess, 154.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 156.1ms\n",
            "Speed: 4.5ms preprocess, 156.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 148.5ms\n",
            "Speed: 6.0ms preprocess, 148.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 146.4ms\n",
            "Speed: 5.3ms preprocess, 146.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 176.9ms\n",
            "Speed: 5.1ms preprocess, 176.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 143.9ms\n",
            "Speed: 5.1ms preprocess, 143.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 149.8ms\n",
            "Speed: 4.2ms preprocess, 149.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.2ms\n",
            "Speed: 4.3ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\n",
            "0: 384x640 3 persons, 154.4ms\n",
            "Speed: 11.0ms preprocess, 154.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.2ms\n",
            "Speed: 6.7ms preprocess, 148.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 180.5ms\n",
            "Speed: 5.5ms preprocess, 180.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 159.3ms\n",
            "Speed: 5.8ms preprocess, 159.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.6ms\n",
            "Speed: 5.1ms preprocess, 151.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 152.2ms\n",
            "Speed: 5.3ms preprocess, 152.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 148.2ms\n",
            "Speed: 5.3ms preprocess, 148.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.4ms\n",
            "Speed: 3.9ms preprocess, 150.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "\n",
            "0: 384x640 2 persons, 152.6ms\n",
            "Speed: 4.9ms preprocess, 152.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 225.9ms\n",
            "Speed: 4.3ms preprocess, 225.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 234.2ms\n",
            "Speed: 5.9ms preprocess, 234.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 243.0ms\n",
            "Speed: 4.1ms preprocess, 243.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 236.1ms\n",
            "Speed: 8.9ms preprocess, 236.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 227.1ms\n",
            "Speed: 7.0ms preprocess, 227.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
            "\n",
            "0: 384x640 1 person, 232.9ms\n",
            "Speed: 5.7ms preprocess, 232.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 244.3ms\n",
            "Speed: 4.1ms preprocess, 244.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.7ms\n",
            "Speed: 4.5ms preprocess, 155.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.1ms\n",
            "Speed: 4.1ms preprocess, 149.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.0ms\n",
            "Speed: 5.8ms preprocess, 147.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 166.4ms\n",
            "Speed: 4.4ms preprocess, 166.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.3ms\n",
            "Speed: 4.9ms preprocess, 151.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
            "\n",
            "0: 384x640 2 persons, 153.2ms\n",
            "Speed: 5.2ms preprocess, 153.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\n",
            "0: 384x640 2 persons, 148.4ms\n",
            "Speed: 4.1ms preprocess, 148.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.4ms\n",
            "Speed: 6.8ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 3 persons, 165.8ms\n",
            "Speed: 4.5ms preprocess, 165.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 144.0ms\n",
            "Speed: 3.9ms preprocess, 144.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 156.6ms\n",
            "Speed: 4.3ms preprocess, 156.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 162.9ms\n",
            "Speed: 5.9ms preprocess, 162.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 145.2ms\n",
            "Speed: 13.2ms preprocess, 145.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 1 person, 163.6ms\n",
            "Speed: 9.5ms preprocess, 163.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 144.8ms\n",
            "Speed: 6.1ms preprocess, 144.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.8ms\n",
            "Speed: 5.7ms preprocess, 150.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.3ms\n",
            "Speed: 4.8ms preprocess, 149.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.3ms\n",
            "Speed: 4.1ms preprocess, 149.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.6ms\n",
            "Speed: 4.4ms preprocess, 146.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 147.9ms\n",
            "Speed: 3.8ms preprocess, 147.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.3ms\n",
            "Speed: 5.1ms preprocess, 151.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
            "\n",
            "0: 384x640 2 persons, 271.5ms\n",
            "Speed: 12.3ms preprocess, 271.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 229.9ms\n",
            "Speed: 4.3ms preprocess, 229.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 237.8ms\n",
            "Speed: 4.4ms preprocess, 237.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 255.7ms\n",
            "Speed: 6.5ms preprocess, 255.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 214.1ms\n",
            "Speed: 3.9ms preprocess, 214.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 151.0ms\n",
            "Speed: 6.6ms preprocess, 151.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.1ms\n",
            "Speed: 6.3ms preprocess, 151.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 150.5ms\n",
            "Speed: 3.8ms preprocess, 150.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\n",
            "0: 384x640 2 persons, 153.9ms\n",
            "Speed: 4.8ms preprocess, 153.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 165.9ms\n",
            "Speed: 4.2ms preprocess, 165.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.7ms\n",
            "Speed: 4.4ms preprocess, 147.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 145.7ms\n",
            "Speed: 4.4ms preprocess, 145.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 171.1ms\n",
            "Speed: 5.6ms preprocess, 171.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.4ms\n",
            "Speed: 5.0ms preprocess, 149.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 1 person, 148.8ms\n",
            "Speed: 5.5ms preprocess, 148.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.8ms\n",
            "Speed: 4.1ms preprocess, 160.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\n",
            "0: 384x640 2 persons, 172.7ms\n",
            "Speed: 5.0ms preprocess, 172.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.0ms\n",
            "Speed: 5.5ms preprocess, 153.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.4ms\n",
            "Speed: 5.5ms preprocess, 149.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 148.8ms\n",
            "Speed: 5.6ms preprocess, 148.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 157.6ms\n",
            "Speed: 4.1ms preprocess, 157.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\n",
            "0: 384x640 1 person, 169.5ms\n",
            "Speed: 4.1ms preprocess, 169.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\n",
            "0: 384x640 (no detections), 150.3ms\n",
            "Speed: 9.9ms preprocess, 150.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 177.7ms\n",
            "Speed: 4.4ms preprocess, 177.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.4ms\n",
            "Speed: 3.0ms preprocess, 152.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 157.4ms\n",
            "Speed: 5.0ms preprocess, 157.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 241.5ms\n",
            "Speed: 8.0ms preprocess, 241.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 238.5ms\n",
            "Speed: 5.0ms preprocess, 238.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 225.4ms\n",
            "Speed: 4.3ms preprocess, 225.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 245.5ms\n",
            "Speed: 4.0ms preprocess, 245.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
            "\n",
            "0: 384x640 (no detections), 225.3ms\n",
            "Speed: 4.1ms preprocess, 225.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 230.7ms\n",
            "Speed: 8.3ms preprocess, 230.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
            "\n",
            "0: 384x640 (no detections), 225.5ms\n",
            "Speed: 16.1ms preprocess, 225.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.8ms\n",
            "Speed: 4.1ms preprocess, 153.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 (no detections), 151.4ms\n",
            "Speed: 4.0ms preprocess, 151.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.3ms\n",
            "Speed: 5.5ms preprocess, 147.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 170.2ms\n",
            "Speed: 5.7ms preprocess, 170.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.3ms\n",
            "Speed: 4.9ms preprocess, 150.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 1 person, 154.9ms\n",
            "Speed: 7.1ms preprocess, 154.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.8ms\n",
            "Speed: 4.4ms preprocess, 150.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.1ms\n",
            "Speed: 5.3ms preprocess, 155.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 1 person, 150.9ms\n",
            "Speed: 5.5ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.0ms\n",
            "Speed: 4.3ms preprocess, 155.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.7ms\n",
            "Speed: 4.0ms preprocess, 155.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.4ms\n",
            "Speed: 4.3ms preprocess, 152.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.5ms\n",
            "Speed: 4.1ms preprocess, 160.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 161.2ms\n",
            "Speed: 4.2ms preprocess, 161.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.4ms\n",
            "Speed: 6.4ms preprocess, 150.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.7ms\n",
            "Speed: 4.2ms preprocess, 151.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 172.9ms\n",
            "Speed: 5.9ms preprocess, 172.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.4ms\n",
            "Speed: 5.0ms preprocess, 150.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.5ms\n",
            "Speed: 5.2ms preprocess, 147.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 146.1ms\n",
            "Speed: 11.0ms preprocess, 146.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.3ms\n",
            "Speed: 5.1ms preprocess, 151.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.4ms\n",
            "Speed: 7.7ms preprocess, 151.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 147.5ms\n",
            "Speed: 5.6ms preprocess, 147.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.7ms\n",
            "Speed: 5.2ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 146.3ms\n",
            "Speed: 4.0ms preprocess, 146.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.8ms\n",
            "Speed: 3.0ms preprocess, 149.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 203.3ms\n",
            "Speed: 8.6ms preprocess, 203.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 233.0ms\n",
            "Speed: 4.2ms preprocess, 233.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
            "\n",
            "0: 384x640 1 person, 232.0ms\n",
            "Speed: 4.8ms preprocess, 232.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 233.6ms\n",
            "Speed: 7.0ms preprocess, 233.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 224.2ms\n",
            "Speed: 3.9ms preprocess, 224.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 232.1ms\n",
            "Speed: 3.9ms preprocess, 232.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 254.1ms\n",
            "Speed: 3.8ms preprocess, 254.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 238.3ms\n",
            "Speed: 3.9ms preprocess, 238.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 167.1ms\n",
            "Speed: 5.3ms preprocess, 167.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.5ms\n",
            "Speed: 3.5ms preprocess, 147.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 2 persons, 167.2ms\n",
            "Speed: 4.5ms preprocess, 167.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 157.5ms\n",
            "Speed: 4.3ms preprocess, 157.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.8ms\n",
            "Speed: 4.2ms preprocess, 152.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 163.5ms\n",
            "Speed: 6.5ms preprocess, 163.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.8ms\n",
            "Speed: 10.0ms preprocess, 150.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.1ms\n",
            "Speed: 8.8ms preprocess, 149.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 150.2ms\n",
            "Speed: 9.7ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 150.0ms\n",
            "Speed: 3.8ms preprocess, 150.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\n",
            "0: 384x640 2 persons, 172.1ms\n",
            "Speed: 8.9ms preprocess, 172.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 164.1ms\n",
            "Speed: 4.6ms preprocess, 164.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 154.1ms\n",
            "Speed: 7.0ms preprocess, 154.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 174.6ms\n",
            "Speed: 4.3ms preprocess, 174.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.6ms\n",
            "Speed: 5.9ms preprocess, 152.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.1ms\n",
            "Speed: 4.0ms preprocess, 151.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 155.3ms\n",
            "Speed: 4.3ms preprocess, 155.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 154.4ms\n",
            "Speed: 5.8ms preprocess, 154.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\n",
            "0: 384x640 1 person, 155.9ms\n",
            "Speed: 4.0ms preprocess, 155.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 175.2ms\n",
            "Speed: 4.2ms preprocess, 175.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 160.7ms\n",
            "Speed: 4.5ms preprocess, 160.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.3ms\n",
            "Speed: 4.4ms preprocess, 149.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 154.5ms\n",
            "Speed: 5.1ms preprocess, 154.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 149.8ms\n",
            "Speed: 4.0ms preprocess, 149.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 151.8ms\n",
            "Speed: 3.8ms preprocess, 151.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 152.4ms\n",
            "Speed: 4.6ms preprocess, 152.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 1 person, 174.2ms\n",
            "Speed: 4.1ms preprocess, 174.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 233.8ms\n",
            "Speed: 3.8ms preprocess, 233.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 236.7ms\n",
            "Speed: 13.3ms preprocess, 236.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 233.9ms\n",
            "Speed: 6.3ms preprocess, 233.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 240.3ms\n",
            "Speed: 6.9ms preprocess, 240.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 222.1ms\n",
            "Speed: 4.0ms preprocess, 222.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 227.5ms\n",
            "Speed: 3.9ms preprocess, 227.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 235.9ms\n",
            "Speed: 8.3ms preprocess, 235.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "\n",
            "0: 384x640 2 persons, 153.1ms\n",
            "Speed: 3.9ms preprocess, 153.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 147.3ms\n",
            "Speed: 8.9ms preprocess, 147.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 146.5ms\n",
            "Speed: 5.3ms preprocess, 146.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 153.7ms\n",
            "Speed: 5.6ms preprocess, 153.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 174.8ms\n",
            "Speed: 3.7ms preprocess, 174.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 152.2ms\n",
            "Speed: 9.1ms preprocess, 152.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 151.2ms\n",
            "Speed: 5.3ms preprocess, 151.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 149.7ms\n",
            "Speed: 8.9ms preprocess, 149.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
            "\n",
            "0: 384x640 2 persons, 154.8ms\n",
            "Speed: 4.0ms preprocess, 154.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 183.3ms\n",
            "Speed: 7.7ms preprocess, 183.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 155.2ms\n",
            "Speed: 3.8ms preprocess, 155.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 151.7ms\n",
            "Speed: 4.9ms preprocess, 151.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 150.1ms\n",
            "Speed: 7.1ms preprocess, 150.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 170.2ms\n",
            "Speed: 3.9ms preprocess, 170.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 150.4ms\n",
            "Speed: 4.7ms preprocess, 150.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 384x640 (no detections), 184.6ms\n",
            "Speed: 6.2ms preprocess, 184.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 (no detections), 158.2ms\n",
            "Speed: 5.1ms preprocess, 158.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 154.1ms\n",
            "Speed: 4.5ms preprocess, 154.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 (no detections), 157.7ms\n",
            "Speed: 4.9ms preprocess, 157.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 179.9ms\n",
            "Speed: 4.7ms preprocess, 179.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.5ms\n",
            "Speed: 4.7ms preprocess, 153.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 156.8ms\n",
            "Speed: 4.6ms preprocess, 156.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 157.5ms\n",
            "Speed: 4.5ms preprocess, 157.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 (no detections), 149.5ms\n",
            "Speed: 5.4ms preprocess, 149.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 (no detections), 157.8ms\n",
            "Speed: 6.2ms preprocess, 157.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 153.3ms\n",
            "Speed: 4.0ms preprocess, 153.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\n",
            "0: 384x640 (no detections), 151.3ms\n",
            "Speed: 4.0ms preprocess, 151.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Person 973 -> Final class: f\n",
            "Person 975 -> Final class: f\n",
            "Person 976 -> Final class: f\n",
            "Person 985 -> Final class: f\n",
            "Person 986 -> Final class: f\n",
            "Person 990 -> Final class: f\n",
            "Person 1008 -> Final class: f\n",
            "Person 1011 -> Final class: f\n",
            "Person 1012 -> Final class: f\n",
            "Person 1014 -> Final class: f\n",
            "Person 1016 -> Final class: f\n",
            "Person 1022 -> Final class: f\n",
            "Person 1025 -> Final class: f\n",
            "Person 1026 -> Final class: f\n",
            "Person 1030 -> Final class: f\n",
            "Person 1031 -> Final class: f\n",
            "Person 1039 -> Final class: f\n",
            "Person 1040 -> Final class: f\n",
            "Person 1042 -> Final class: f\n",
            "Person 1044 -> Final class: f\n",
            "Person 1046 -> Final class: f\n",
            "Person 1047 -> Final class: f\n",
            "Person 1052 -> Final class: a\n",
            "Person 1053 -> Final class: f\n",
            "Person 1059 -> Final class: f\n",
            "Person 1062 -> Final class: f\n",
            "Person 1065 -> Final class: f\n",
            "Person 1066 -> Final class: f\n",
            "Person 1070 -> Final class: f\n",
            "Person 1071 -> Final class: f\n",
            "Person 1086 -> Final class: f\n",
            "Person 1087 -> Final class: f\n",
            "Person 1096 -> Final class: f\n",
            "Person 1098 -> Final class: f\n",
            "Person 1103 -> Final class: f\n",
            "Person 1106 -> Final class: f\n",
            "Person 1110 -> Final class: f\n",
            "Person 1120 -> Final class: f\n",
            "Person 1122 -> Final class: f\n",
            "Person 1129 -> Final class: f\n",
            "Person 1131 -> Final class: f\n",
            "Person 1136 -> Final class: f\n",
            "Person 1128 -> Final class: f\n",
            "Person 1144 -> Final class: f\n",
            "Person 1142 -> Final class: f\n",
            "Person 1148 -> Final class: f\n",
            "Person 1149 -> Final class: f\n",
            "Person 1150 -> Final class: f\n",
            "Person 1151 -> Final class: f\n",
            "Person 1159 -> Final class: f\n",
            "Person 1164 -> Final class: f\n",
            "Person 1165 -> Final class: f\n",
            "Person 1166 -> Final class: f\n",
            "Person 1167 -> Final class: f\n",
            "Person 1168 -> Final class: f\n",
            "Person 1171 -> Final class: f\n",
            "Person 1177 -> Final class: f\n",
            "Person 1178 -> Final class: f\n",
            "Person 1182 -> Final class: f\n",
            "Person 1199 -> Final class: f\n",
            "Person 1210 -> Final class: f\n",
            "Person 1214 -> Final class: f\n",
            "Person 1215 -> Final class: f\n",
            "Person 1223 -> Final class: f\n",
            "Person 1224 -> Final class: f\n",
            "Person 1231 -> Final class: f\n",
            "Person 1248 -> Final class: f\n",
            "Person 1250 -> Final class: f\n",
            "Person 1258 -> Final class: f\n",
            "Person 1269 -> Final class: f\n",
            "Person 1279 -> Final class: f\n",
            "Person 1284 -> Final class: f\n",
            "Person 1292 -> Final class: f\n",
            "Person 1298 -> Final class: f\n",
            "Person 1300 -> Final class: f\n",
            "Person 1307 -> Final class: f\n",
            "Person 1309 -> Final class: f\n",
            "Person 1311 -> Final class: f\n",
            "Person 1318 -> Final class: f\n",
            "Person 1319 -> Final class: f\n",
            "Person 1323 -> Final class: f\n",
            "Person 1325 -> Final class: f\n",
            "Person 1335 -> Final class: f\n",
            "Person 1337 -> Final class: f\n",
            "Person 1338 -> Final class: a\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'973': 'f',\n",
              " '975': 'f',\n",
              " '976': 'f',\n",
              " '985': 'f',\n",
              " '986': 'f',\n",
              " '990': 'f',\n",
              " '1008': 'f',\n",
              " '1011': 'f',\n",
              " '1012': 'f',\n",
              " '1014': 'f',\n",
              " '1016': 'f',\n",
              " '1022': 'f',\n",
              " '1025': 'f',\n",
              " '1026': 'f',\n",
              " '1030': 'f',\n",
              " '1031': 'f',\n",
              " '1039': 'f',\n",
              " '1040': 'f',\n",
              " '1042': 'f',\n",
              " '1044': 'f',\n",
              " '1046': 'f',\n",
              " '1047': 'f',\n",
              " '1052': 'a',\n",
              " '1053': 'f',\n",
              " '1059': 'f',\n",
              " '1062': 'f',\n",
              " '1065': 'f',\n",
              " '1066': 'f',\n",
              " '1070': 'f',\n",
              " '1071': 'f',\n",
              " '1086': 'f',\n",
              " '1087': 'f',\n",
              " '1096': 'f',\n",
              " '1098': 'f',\n",
              " '1103': 'f',\n",
              " '1106': 'f',\n",
              " '1110': 'f',\n",
              " '1120': 'f',\n",
              " '1122': 'f',\n",
              " '1129': 'f',\n",
              " '1131': 'f',\n",
              " '1136': 'f',\n",
              " '1128': 'f',\n",
              " '1144': 'f',\n",
              " '1142': 'f',\n",
              " '1148': 'f',\n",
              " '1149': 'f',\n",
              " '1150': 'f',\n",
              " '1151': 'f',\n",
              " '1159': 'f',\n",
              " '1164': 'f',\n",
              " '1165': 'f',\n",
              " '1166': 'f',\n",
              " '1167': 'f',\n",
              " '1168': 'f',\n",
              " '1171': 'f',\n",
              " '1177': 'f',\n",
              " '1178': 'f',\n",
              " '1182': 'f',\n",
              " '1199': 'f',\n",
              " '1210': 'f',\n",
              " '1214': 'f',\n",
              " '1215': 'f',\n",
              " '1223': 'f',\n",
              " '1224': 'f',\n",
              " '1231': 'f',\n",
              " '1248': 'f',\n",
              " '1250': 'f',\n",
              " '1258': 'f',\n",
              " '1269': 'f',\n",
              " '1279': 'f',\n",
              " '1284': 'f',\n",
              " '1292': 'f',\n",
              " '1298': 'f',\n",
              " '1300': 'f',\n",
              " '1307': 'f',\n",
              " '1309': 'f',\n",
              " '1311': 'f',\n",
              " '1318': 'f',\n",
              " '1319': 'f',\n",
              " '1323': 'f',\n",
              " '1325': 'f',\n",
              " '1335': 'f',\n",
              " '1337': 'f',\n",
              " '1338': 'a'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GMu8XZ5cpj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ht_sMepGki_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uilO8MtWki8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8OwoLJyki0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mhG1Cc_kixY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "webcam use"
      ],
      "metadata": {
        "id": "YI7SIGvnkj1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(0)  # 0 = default webcam\n"
      ],
      "metadata": {
        "id": "CXBOoty6klcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_webcam_live():\n",
        "    import cv2\n",
        "    from collections import Counter\n",
        "\n",
        "    tracks_dict = {}\n",
        "    tracks_preds = {}\n",
        "\n",
        "    cap = cv2.VideoCapture(0)  # use webcam\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        # Skip frames to reduce processing if needed\n",
        "        if frame_count % 2 != 0:  # process every 2nd frame (adjust as needed)\n",
        "            continue\n",
        "\n",
        "        # Detect humans\n",
        "        results = yolo_model.predict(frame, classes=[0])\n",
        "        detections = []\n",
        "        for r in results:\n",
        "            for box in r.boxes.xyxy.cpu().numpy():\n",
        "                x1, y1, x2, y2 = map(int, box[:4])\n",
        "                detections.append(([x1, y1, x2-x1, y2-y1], 1.0, 'person'))\n",
        "\n",
        "        # Track humans\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            track_id = track.track_id\n",
        "            l, t, r, b = map(int, track.to_ltrb())\n",
        "            crop = frame[t:b, l:r]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop_resized = cv2.resize(crop, img_size) / 255.0\n",
        "\n",
        "            if track_id not in tracks_dict:\n",
        "                tracks_dict[track_id] = []\n",
        "                tracks_preds[track_id] = []\n",
        "\n",
        "            tracks_dict[track_id].append(crop_resized)\n",
        "\n",
        "            if len(tracks_dict[track_id]) >= num_frames:\n",
        "                clip = np.array(tracks_dict[track_id][:num_frames])\n",
        "                clip = np.expand_dims(clip, axis=0)\n",
        "                pred = model.predict(clip)\n",
        "                tracks_preds[track_id].append(np.argmax(pred))\n",
        "                tracks_dict[track_id] = tracks_dict[track_id][8:]\n",
        "\n",
        "            # Draw live bounding box + label\n",
        "            if len(tracks_preds[track_id]) > 0:\n",
        "                most_common = Counter(tracks_preds[track_id]).most_common(1)[0][0]\n",
        "                label = class_names[most_common]\n",
        "                cv2.rectangle(frame, (l, t), (r, b), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"ID {track_id}: {label}\", (l, t-10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "        # Show live webcam\n",
        "        cv2.imshow(\"Webcam Live Detection\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Final class per person\n",
        "    final_classes = {}\n",
        "    for track_id, preds in tracks_preds.items():\n",
        "        if len(preds) == 0:\n",
        "            continue\n",
        "        most_common = Counter(preds).most_common(1)[0][0]\n",
        "        final_classes[track_id] = class_names[most_common]\n",
        "\n",
        "    print(\"Final classes per person:\", final_classes)\n",
        "    return final_classes\n"
      ],
      "metadata": {
        "id": "KGCD8OvWkpBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_results = process_webcam_live()\n",
        "\n"
      ],
      "metadata": {
        "id": "o_X7dLkNkpxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}